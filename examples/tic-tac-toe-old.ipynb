{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPipe client initialized\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from openpipe.client import OpenPipe\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "op_client = OpenPipe()\n",
    "print(\"OpenPipe client initialized\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "api = art.LocalAPI(wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api._get_or_create_model(\n",
    "    name=\"tic-tac-toe-002\",\n",
    "    base_model=\"NousResearch/Hermes-2-Theta-Llama-3-8B\",\n",
    "    _config={\"init_args\": {\"max_seq_length\": 8192, \"num_scheduler_steps\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:496: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  left = re.match(\"[\\s\\n]{4,}\", leftover).span()[1]\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:924: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  .replace(\"*\", \"\\*\").replace(\"^\", \"\\^\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:924: SyntaxWarning: invalid escape sequence '\\^'\n",
      "  .replace(\"*\", \"\\*\").replace(\"^\", \"\\^\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:925: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  .replace(\"-\", \"\\-\").replace(\"_\", \"\\_\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:925: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  .replace(\"-\", \"\\-\").replace(\"_\", \"\\_\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:926: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  .replace(\":\", \"\\:\").replace(\"+\", \"\\+\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:926: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  .replace(\":\", \"\\:\").replace(\"+\", \"\\+\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:927: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  .replace(\".\", \"\\.\").replace(\",\", \"\\,\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:927: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  .replace(\".\", \"\\.\").replace(\",\", \"\\,\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:928: SyntaxWarning: invalid escape sequence '\\('\n",
      "  .replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:928: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  .replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:929: SyntaxWarning: invalid escape sequence '\\['\n",
      "  .replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:929: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  .replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1018: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  if \"loss\\_function\" in cross_entropy_find and \"loss_function\" not in forward:\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1020: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  elif \"loss\\_function\" not in cross_entropy_find and \"loss_function\" in forward:\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1056: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if \"logits = outputs\\.logits\" in cross_entropy_find:\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1163: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  r\"for ([^\\s]{1,}) in \" + modulelist_item + \"\\:[\\n]\" + \\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1406: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_find = f\"{call_class}\\(([^\\)]{{1,}})\\)\"\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1406: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  regex_find = f\"{call_class}\\(([^\\)]{{1,}})\\)\"\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1412: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_find = \"def forward\\(([^\\)]{1,})\\)\"\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1518: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  inherited_modules = re.findall(r\"class ([^\\s]{1,})\\(\" + inherited_class + \"\\)\", full_source)\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1572: SyntaxWarning: invalid escape sequence '\\('\n",
      "  called = re.findall(r\"[\\s]{1,}\" + re.escape(function) + \"\\(.+?\\)\", full_source, flags = re.DOTALL)\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/peft_utils.py:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  name = re.sub(\"\\.([\\d]{1,})\\.\", r\"[\\1].\", name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-09 16:26:31 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading NousResearch/Hermes-2-Theta-Llama-3-8B with actual GPU utilization = 78.4%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 8192. Num Sequences = 226.\n",
      "Unsloth: vLLM's KV Cache can use up to 47.03 GB. Also swap space = 6 GB.\n",
      "INFO 04-09 16:26:41 config.py:549] This model supports multiple tasks: {'reward', 'score', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-09 16:26:41 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='NousResearch/Hermes-2-Theta-Llama-3-8B', speculative_config=None, tokenizer='NousResearch/Hermes-2-Theta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=NousResearch/Hermes-2-Theta-Llama-3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":232}, use_cached_outputs=False, \n",
      "INFO 04-09 16:26:43 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-09 16:26:43 model_runner.py:1110] Starting to load model NousResearch/Hermes-2-Theta-Llama-3-8B...\n",
      "INFO 04-09 16:26:43 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 04-09 16:26:43 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 04-09 16:28:09 weight_utils.py:270] Time spent downloading weights for NousResearch/Hermes-2-Theta-Llama-3-8B: 85.951296 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  4.43it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.67it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.38it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.42it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 16:28:13 model_runner.py:1115] Loading model weights took 5.3168 GB\n",
      "INFO 04-09 16:28:13 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-09 16:28:22 worker.py:267] Memory profiling takes 8.86 seconds\n",
      "INFO 04-09 16:28:22 worker.py:267] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.78) = 62.02GiB\n",
      "INFO 04-09 16:28:22 worker.py:267] model weights take 5.32GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 1.12GiB; the rest of the memory reserved for KV Cache is 55.43GiB.\n",
      "INFO 04-09 16:28:22 executor_base.py:111] # cuda blocks: 28380, # CPU blocks: 3072\n",
      "INFO 04-09 16:28:22 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 55.43x\n",
      "INFO 04-09 16:28:25 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:21<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-09 16:28:46 model_runner.py:1562] Graph capturing finished in 21 secs, took 4.36 GiB\n",
      "INFO 04-09 16:28:46 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 33.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "base_model.model.lm_head.weight is unsupported LoRA weight",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 268\u001b[39m\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trajectory\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m model.get_iteration(), \u001b[32m50\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m model.openai_client(\n\u001b[32m    269\u001b[39m         estimated_completion_tokens=\u001b[32m100\u001b[39m, verbosity=\u001b[32m2\u001b[39m\n\u001b[32m    270\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m openai_client:\n\u001b[32m    271\u001b[39m         val_groups, train_groups = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    272\u001b[39m             art.gather_trajectories(\n\u001b[32m    273\u001b[39m                 (\n\u001b[32m   (...)\u001b[39m\u001b[32m    288\u001b[39m             ),\n\u001b[32m    289\u001b[39m         )\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m model.log(val_groups)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/model.py:22\u001b[39m, in \u001b[36mClientWrapper.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncOpenAI:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_client()\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/model.py:102\u001b[39m, in \u001b[36mModel._openai_client.<locals>.get_client\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_client\u001b[39m() -> AsyncOpenAI:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     client, semaphore = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api._get_openai_client(\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mself\u001b[39m, estimated_completion_tokens, tool_use, verbosity, _config\n\u001b[32m    104\u001b[39m     )\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m patch_openai(client, semaphore, \u001b[38;5;28mself\u001b[39m.api._close_openai_client)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/local/api.py:234\u001b[39m, in \u001b[36mLocalAPI._get_openai_client\u001b[39m\u001b[34m(self, model, estimated_completion_tokens, tool_use, verbosity, config)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_openai_client\u001b[39m(\n\u001b[32m    226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    227\u001b[39m     model: Model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m     config: OpenAIServerConfig | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    232\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[AsyncOpenAI, asyncio.Semaphore]:\n\u001b[32m    233\u001b[39m     service = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_service(model)\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m service.start_openai_server(tool_use=tool_use, config=config)\n\u001b[32m    235\u001b[39m     server_args = (config \u001b[38;5;129;01mor\u001b[39;00m {}).get(\u001b[33m\"\u001b[39m\u001b[33mserver_args\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    237\u001b[39m         AsyncOpenAI(\n\u001b[32m    238\u001b[39m             base_url=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_args.get(\u001b[33m'\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m0.0.0.0\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_args.get(\u001b[33m'\u001b[39m\u001b[33mport\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[32m8000\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m         asyncio.Semaphore(\u001b[32m1024\u001b[39m),\n\u001b[32m    248\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/mp_actors/traceback.py:27\u001b[39m, in \u001b[36mstreamline_tracebacks.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(streamlined_traceback())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/local/service.py:54\u001b[39m, in \u001b[36mstart_openai_server\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.trainer.save_model(lora_path)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_openai_server()\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28mself\u001b[39m._openai_server_task = \u001b[38;5;28;01mawait\u001b[39;00m openai_server_task(\n\u001b[32m     55\u001b[39m     state=\u001b[38;5;28mself\u001b[39m.state.vllm,\n\u001b[32m     56\u001b[39m     config=get_openai_server_config(\n\u001b[32m     57\u001b[39m         model_name=\u001b[38;5;28mself\u001b[39m.model_name,\n\u001b[32m     58\u001b[39m         base_model=\u001b[38;5;28mself\u001b[39m.base_model,\n\u001b[32m     59\u001b[39m         log_file=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/logs/vllm.log\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m         lora_path=lora_path,\n\u001b[32m     61\u001b[39m         tool_use=tool_use,\n\u001b[32m     62\u001b[39m         config=config,\n\u001b[32m     63\u001b[39m     ),\n\u001b[32m     64\u001b[39m )\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m._set_lora(lora_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/local/vllm.py:81\u001b[39m, in \u001b[36mopenai_server_task\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     79\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to reach OpenAI-compatible server in time.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         task.result()\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m openai_server_task\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mresult\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36m__step_run_and_handle_result\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py:951\u001b[39m, in \u001b[36mrun_server\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    948\u001b[39m app = build_app(args)\n\u001b[32m    950\u001b[39m model_config = \u001b[38;5;28;01mawait\u001b[39;00m engine_client.get_model_config()\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m init_app_state(engine_client, model_config, app.state, args)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_listen_addr\u001b[39m(a: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_ipv6_address(a):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py:830\u001b[39m, in \u001b[36minit_app_state\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    820\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mUsing supplied chat template:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    821\u001b[39m                 resolved_chat_template)\n\u001b[32m    823\u001b[39m state.openai_serving_models = OpenAIServingModels(\n\u001b[32m    824\u001b[39m     engine_client=engine_client,\n\u001b[32m    825\u001b[39m     model_config=model_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    828\u001b[39m     prompt_adapters=args.prompt_adapters,\n\u001b[32m    829\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m state.openai_serving_models.init_static_loras()\n\u001b[32m    831\u001b[39m state.openai_serving_chat = OpenAIServingChat(\n\u001b[32m    832\u001b[39m     engine_client,\n\u001b[32m    833\u001b[39m     model_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    844\u001b[39m     enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n\u001b[32m    845\u001b[39m ) \u001b[38;5;28;01mif\u001b[39;00m model_config.runner_type == \u001b[33m\"\u001b[39m\u001b[33mgenerate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    846\u001b[39m state.openai_serving_completion = OpenAIServingCompletion(\n\u001b[32m    847\u001b[39m     engine_client,\n\u001b[32m    848\u001b[39m     model_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    851\u001b[39m     return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n\u001b[32m    852\u001b[39m ) \u001b[38;5;28;01mif\u001b[39;00m model_config.runner_type == \u001b[33m\"\u001b[39m\u001b[33mgenerate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/vllm/entrypoints/openai/serving_models.py:96\u001b[39m, in \u001b[36minit_static_loras\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     93\u001b[39m load_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_lora_adapter(\n\u001b[32m     94\u001b[39m     request=load_request, base_model_name=lora.base_model_name)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(load_result, ErrorResponse):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(load_result.message)\n",
      "\u001b[31mValueError\u001b[39m: base_model.model.lm_head.weight is unsupported LoRA weight"
     ]
    }
   ],
   "source": [
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "from typing import TypedDict\n",
    "from openpipe.client import OpenPipe\n",
    "import time\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TicTacToeGame(TypedDict):\n",
    "    board: list[list[str]]\n",
    "    agent_symbol: Literal[\"x\", \"o\"]\n",
    "    opponent_symbol: Literal[\"x\", \"o\"]\n",
    "\n",
    "\n",
    "def generate_game(board_length: int = 3) -> TicTacToeGame:\n",
    "    board = [[\"_\" for _ in range(board_length)] for _ in range(board_length)]\n",
    "    agent_symbol = random.choice([\"x\", \"o\"])\n",
    "    opponent_symbol = \"x\" if agent_symbol == \"o\" else \"o\"\n",
    "    return {\n",
    "        \"board\": board,\n",
    "        \"agent_symbol\": agent_symbol,\n",
    "        \"opponent_symbol\": opponent_symbol,\n",
    "    }\n",
    "\n",
    "\n",
    "def render_board(game: TicTacToeGame) -> str:\n",
    "    board = game[\"board\"]\n",
    "    board_length = len(board)\n",
    "    # print something like this:\n",
    "    #    1   2   3\n",
    "    # A  _ | x | x\n",
    "    # B  o | _ | _\n",
    "    # C  _ | o | _\n",
    "    # where _ is an empty cell\n",
    "\n",
    "    board_str = \"   \" + \"   \".join([str(i + 1) for i in range(board_length)]) + \"\\n\"\n",
    "    for i in range(board_length):\n",
    "        board_str += f\"{chr(65 + i)}  {board[i][0]} | {board[i][1]} | {board[i][2]}\\n\"\n",
    "    return board_str\n",
    "\n",
    "\n",
    "def get_opponent_move(game: TicTacToeGame) -> tuple[int, int]:\n",
    "    # get a random empty cell\n",
    "    empty_cells = [\n",
    "        (i, j) for i in range(3) for j in range(3) if game[\"board\"][i][j] == \"_\"\n",
    "    ]\n",
    "    return random.choice(empty_cells)\n",
    "\n",
    "\n",
    "class AgentMove(BaseModel):\n",
    "    reason: str\n",
    "    square: str\n",
    "\n",
    "\n",
    "def apply_agent_move(game: TicTacToeGame, move: str) -> None:\n",
    "    board_length = len(game[\"board\"])\n",
    "    json_move = json.loads(move)\n",
    "    square = json_move[\"square\"]\n",
    "\n",
    "    try:\n",
    "        row_index = ord(square[0]) - 65\n",
    "        col_index = int(square[1]) - 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise ValueError(\"Unable to parse square\")\n",
    "\n",
    "    if (\n",
    "        row_index < 0\n",
    "        or row_index >= board_length\n",
    "        or col_index < 0\n",
    "        or col_index >= board_length\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Invalid move, row or column out of bounds: {row_index}, {col_index}\"\n",
    "        )\n",
    "\n",
    "    # check if the move is valid\n",
    "    if game[\"board\"][row_index][col_index] != \"_\":\n",
    "        raise ValueError(\"Square already occupied\")\n",
    "\n",
    "    game[\"board\"][row_index][col_index] = game[\"agent_symbol\"]\n",
    "\n",
    "\n",
    "def check_winner(board: list[list[str]]) -> Literal[\"x\", \"o\", \"draw\", None]:\n",
    "    board_length = len(board)\n",
    "    # check rows\n",
    "    for row in board:\n",
    "        if row.count(row[0]) == board_length and row[0] != \"_\":\n",
    "            return row[0]\n",
    "    # check columns\n",
    "    for col in range(board_length):\n",
    "        if [board[row][col] for row in range(board_length)].count(\n",
    "            board[0][col]\n",
    "        ) == board_length and board[0][col] != \"_\":\n",
    "            return board[0][col]\n",
    "\n",
    "    # top right to bottom left\n",
    "    upward_diagonal = [board[i][board_length - i - 1] for i in range(board_length)]\n",
    "    if (\n",
    "        upward_diagonal.count(upward_diagonal[0]) == board_length\n",
    "        and upward_diagonal[0] != \"_\"\n",
    "    ):\n",
    "        return upward_diagonal[0]\n",
    "\n",
    "    # top left to bottom right\n",
    "    downward_diagonal = [board[i][i] for i in range(board_length)]\n",
    "    if (\n",
    "        downward_diagonal.count(downward_diagonal[0]) == board_length\n",
    "        and downward_diagonal[0] != \"_\"\n",
    "    ):\n",
    "        return downward_diagonal[0]\n",
    "\n",
    "    # check for draw\n",
    "    if all(cell != \"_\" for row in board for cell in row):\n",
    "        return \"draw\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_trajectory_messages(trajectory: art.Trajectory) -> art.Messages:\n",
    "    messages: art.Messages = []\n",
    "    for item in trajectory.messages_and_choices:\n",
    "\n",
    "        # if item is not a dict, convert it to a dict\n",
    "        if not isinstance(item, dict):\n",
    "            item = item.to_dict()\n",
    "\n",
    "        # check if item is a choice\n",
    "        if \"message\" in item:\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": item[\"message\"][\"content\"]}\n",
    "            )\n",
    "        else:\n",
    "            # otherwise it's a message\n",
    "            messages.append(item)\n",
    "    return messages\n",
    "\n",
    "\n",
    "failing_trajectory = None\n",
    "\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError,))\n",
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, iteration: int, is_validation: bool\n",
    ") -> art.Trajectory:\n",
    "\n",
    "    game = generate_game()\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return the move in the format 'A1', 'B2', 'C3', etc. You are the {game['agent_symbol']} symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        reward=0,\n",
    "        metrics={\"test\": 5},\n",
    "    )\n",
    "\n",
    "    if game[\"agent_symbol\"] == \"o\":\n",
    "        starting_opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
    "            \"opponent_symbol\"\n",
    "        ]\n",
    "\n",
    "    while check_winner(game[\"board\"]) is None:\n",
    "\n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = get_trajectory_messages(trajectory)\n",
    "\n",
    "        async def get_completion():\n",
    "            return await client.chat.completions.create(\n",
    "                max_completion_tokens=2048,\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "                extra_body={\"guided_json\": AgentMove.model_json_schema()},\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            op_client.report(\n",
    "                requested_at=requested_at,\n",
    "                received_at=int(time.time() * 1000),\n",
    "                req_payload={\n",
    "                    \"model\": model.name,\n",
    "                    \"messages\": messages,\n",
    "                    \"metadata\": {\n",
    "                        \"notebook-id\": \"tic-tac-toe\",\n",
    "                        \"iteration\": str(iteration),\n",
    "                        \"validation\": str(is_validation),\n",
    "                        \"move_number\": str(len(trajectory.messages_and_choices) - 1),\n",
    "                    },\n",
    "                },\n",
    "                resp_payload=chat_completion,\n",
    "                status_code=200,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "        except ValueError as e:\n",
    "            trajectory.reward = -1\n",
    "            break\n",
    "\n",
    "        if check_winner(game[\"board\"]) is not None:\n",
    "            break\n",
    "\n",
    "        opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
    "\n",
    "    winner = check_winner(game[\"board\"])\n",
    "\n",
    "    if winner == game[\"agent_symbol\"]:\n",
    "        trajectory.reward = 1\n",
    "    elif winner == game[\"opponent_symbol\"]:\n",
    "        trajectory.reward = 0\n",
    "    elif winner == \"draw\":\n",
    "        trajectory.reward = 0.5\n",
    "\n",
    "    try:\n",
    "        op_client.update_log_metadata(\n",
    "            filters=[\n",
    "                {\n",
    "                    \"field\": \"completionId\",\n",
    "                    \"equals\": last_completion.id,\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"reward\": str(trajectory.reward),\n",
    "                \"reward_assigned\": \"true\",\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "openai_client = await model.openai_client()\n",
    "for i in range(await model.get_iteration(), 50):\n",
    "    val_groups, train_groups = await asyncio.gather(\n",
    "        art.gather_trajectories(\n",
    "            (\n",
    "                (rollout(openai_client, i, is_validation=True) for _ in range(64))\n",
    "                for _ in range(1)\n",
    "            ),\n",
    "            pbar_desc=\"val\",\n",
    "            stream_chat_completions=8,\n",
    "            return_exceptions=False,\n",
    "        ),\n",
    "        art.gather_trajectories(\n",
    "            (\n",
    "                (rollout(openai_client, i, is_validation=False) for _ in range(512))\n",
    "                for _ in range(1)\n",
    "            ),\n",
    "            pbar_desc=\"train\",\n",
    "            return_exceptions=False,\n",
    "        ),\n",
    "    )\n",
    "    await model.log(val_groups)\n",
    "    await model.clear_iterations()\n",
    "    await model.tune(\n",
    "        train_groups, config=art.TuneConfig(plot_tensors=True, verbosity=2)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
