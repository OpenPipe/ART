{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from typing import TypedDict\n",
    "from typing import Literal\n",
    "import string\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "WINNING_VALUE = 256\n",
    "\n",
    "\n",
    "# Class that keeps track of state for a single game of 2048\n",
    "class TwentyFortyEightGame(TypedDict):\n",
    "    id: str\n",
    "    board: list[list[int | None]]\n",
    "\n",
    "\n",
    "# Randomly populates a cell on the board with a 2 or 4\n",
    "def populate_random_cell(game: TwentyFortyEightGame) -> None:\n",
    "    all_clear_coordinates = [\n",
    "        (i, j)\n",
    "        for i in range(len(game[\"board\"]))\n",
    "        for j in range(len(game[\"board\"][i]))\n",
    "        if game[\"board\"][i][j] is None\n",
    "    ]\n",
    "    random_clear_coordinates = random.choice(all_clear_coordinates)\n",
    "    # 90% chance to populate a 2, 10% chance to populate a 4\n",
    "    game[\"board\"][random_clear_coordinates[0]][random_clear_coordinates[1]] = (\n",
    "        2 if random.random() < 0.9 else 4\n",
    "    )\n",
    "\n",
    "\n",
    "# Generates a new game of 2048\n",
    "def generate_game(board_length: int = 4) -> TwentyFortyEightGame:\n",
    "    # random 6 character string\n",
    "    id = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
    "    game = {\n",
    "        \"id\": id,\n",
    "        \"board\": [[None for _ in range(board_length)] for _ in range(board_length)],\n",
    "    }\n",
    "\n",
    "    # populate two random cells\n",
    "    populate_random_cell(game)\n",
    "    populate_random_cell(game)\n",
    "\n",
    "    return game\n",
    "\n",
    "\n",
    "# Renders the board in a human-readable format\n",
    "def render_board(game: TwentyFortyEightGame) -> str:\n",
    "    board = game[\"board\"]\n",
    "    # print something like this:\n",
    "    # _    | 2    | _    | 4\n",
    "    # 4    | 8    | 2    | 16\n",
    "    # 16   | 32   | 64   | 128\n",
    "    # _    | 2    | 2    | 4\n",
    "    # where _ is an empty cell\n",
    "\n",
    "    max_cell_width = max(\n",
    "        [len(str(cell)) for row in board for cell in row if cell is not None]\n",
    "    )\n",
    "\n",
    "    board_str = \"\"\n",
    "    for row in board:\n",
    "        # pad the cells with spaces to make them the same width\n",
    "        board_str += \"|\".join(\n",
    "            [\n",
    "                str(cell).rjust(max_cell_width)\n",
    "                if cell is not None\n",
    "                else \"_\".rjust(max_cell_width)\n",
    "                for cell in row\n",
    "            ]\n",
    "        )\n",
    "        board_str += \"\\n\"\n",
    "    return board_str\n",
    "\n",
    "\n",
    "# condense, privileging matches at the start of the sequence\n",
    "# sequences should be passed starting with cells that are the furthest in the direction in which the board is being condensed\n",
    "def condense_sequence(sequence: list[int | None]) -> list[int | None]:\n",
    "    condensed_sequence = []\n",
    "\n",
    "    gapless_sequence = [cell for cell in sequence if cell is not None]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(gapless_sequence):\n",
    "        if (\n",
    "            i + 1 < len(gapless_sequence)\n",
    "            and gapless_sequence[i] == gapless_sequence[i + 1]\n",
    "        ):\n",
    "            condensed_sequence.append(gapless_sequence[i] * 2)\n",
    "            i += 2\n",
    "        else:\n",
    "            condensed_sequence.append(gapless_sequence[i])\n",
    "            i += 1\n",
    "\n",
    "    # pad the sequence with None at the end\n",
    "    return condensed_sequence + [None] * (4 - len(condensed_sequence))\n",
    "\n",
    "\n",
    "# Condenses the board in a given direction\n",
    "def condense_board(\n",
    "    game: TwentyFortyEightGame, direction: Literal[\"left\", \"right\", \"up\", \"down\"]\n",
    ") -> None:\n",
    "    if direction == \"left\":\n",
    "        for row in game[\"board\"]:\n",
    "            condensed_row = condense_sequence(row)\n",
    "            for i in range(len(row)):\n",
    "                row[i] = condensed_row[i]\n",
    "\n",
    "    if direction == \"right\":\n",
    "        for row in game[\"board\"]:\n",
    "            reversed_row = row[::-1]\n",
    "            # reverse the row before and after condensing\n",
    "            condensed_row = condense_sequence(reversed_row)[::-1]\n",
    "            for i in range(len(row)):\n",
    "                row[i] = condensed_row[i]\n",
    "\n",
    "    if direction == \"up\":\n",
    "        for col_index in range(len(game[\"board\"][0])):\n",
    "            column = [row[col_index] for row in game[\"board\"]]\n",
    "\n",
    "            condensed_column = condense_sequence(column)\n",
    "            for row_index in range(len(column)):\n",
    "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
    "\n",
    "    if direction == \"down\":\n",
    "        for col_index in range(len(game[\"board\"][0])):\n",
    "            column = [row[col_index] for row in game[\"board\"]]\n",
    "            reversed_column = column[::-1]\n",
    "            condensed_column = condense_sequence(reversed_column)[::-1]\n",
    "            for row_index in range(len(column)):\n",
    "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
    "\n",
    "\n",
    "# Applies an agent move to the game board\n",
    "def apply_agent_move(game: TwentyFortyEightGame, move_xml: str) -> None:\n",
    "    direction = None\n",
    "    # parse the move\n",
    "    try:\n",
    "        root = ET.fromstring(move_xml)\n",
    "        direction = root.text\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Invalid xml\")\n",
    "\n",
    "    if direction not in [\"left\", \"right\", \"up\", \"down\"]:\n",
    "        raise ValueError(\"Invalid direction\")\n",
    "\n",
    "    condense_board(game, direction)\n",
    "\n",
    "    populate_random_cell(game)\n",
    "\n",
    "\n",
    "# Returns the maximum cell value on the board\n",
    "def max_cell_value(game: TwentyFortyEightGame) -> int:\n",
    "    return max([cell for row in game[\"board\"] for cell in row if cell is not None])\n",
    "\n",
    "\n",
    "# Returns True if the game is finished\n",
    "def check_game_finished(game: TwentyFortyEightGame) -> bool:\n",
    "    if max_cell_value(game) >= WINNING_VALUE:\n",
    "        return True\n",
    "\n",
    "    # check if any cell is empty\n",
    "    if any(cell is None for row in game[\"board\"] for cell in row):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Returns the sum of all the cell values on the board\n",
    "def total_board_value(game: TwentyFortyEightGame) -> int:\n",
    "    return sum([cell for row in game[\"board\"] for cell in row if cell is not None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art.local import LocalAPI\n",
    "from dotenv import load_dotenv\n",
    "from openpipe.client import AsyncOpenPipe\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the server\n",
    "api = LocalAPI()\n",
    "\n",
    "REGISTER_TRAINABLE_MODEL = False\n",
    "if REGISTER_TRAINABLE_MODEL:\n",
    "    # Declare the model\n",
    "    model = art.TrainableModel(\n",
    "        name=\"012\",\n",
    "        project=\"2048-dev\",\n",
    "        base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        # To run on a T4, we need to override some config defaults.\n",
    "        _internal_config=art.dev.InternalModelConfig(\n",
    "            init_args=art.dev.InitArgs(\n",
    "                max_seq_length=8192,\n",
    "            ),\n",
    "            engine_args=art.dev.EngineArgs(\n",
    "                enforce_eager=True,\n",
    "                gpu_memory_utilization=0.8,\n",
    "                num_scheduler_steps=1,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    await model.register(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError, requests.ReadTimeout))\n",
    "async def rollout(\n",
    "    model: art.Model, step: int, is_validation: bool\n",
    ") -> art.Trajectory:\n",
    "    game = generate_game()\n",
    "\n",
    "    move_number = 0\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\",\n",
    "            }\n",
    "        ],\n",
    "        reward=0,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = trajectory.messages()\n",
    "\n",
    "        async def get_completion():\n",
    "            client = model.openai_client()\n",
    "            return await client.chat.completions.create(\n",
    "                max_completion_tokens=128,\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\", e)\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            if op_client.api_key:\n",
    "                await op_client.report(\n",
    "                    requested_at=requested_at,\n",
    "                    received_at=int(time.time() * 1000),\n",
    "                    req_payload={\n",
    "                        \"model\": model.name,\n",
    "                        \"messages\": messages,\n",
    "                        \"metadata\": {\n",
    "                            \"game_id\": game[\"id\"],\n",
    "                            \"notebook-id\": \"2048\",\n",
    "                            \"step\": str(step),\n",
    "                            \"validation\": str(is_validation),\n",
    "                            \"move_number\": str(move_number),\n",
    "                        },\n",
    "                    },\n",
    "                    resp_payload=chat_completion,\n",
    "                    status_code=200,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "            move_number += 1\n",
    "        except ValueError:\n",
    "            trajectory.reward = -1\n",
    "            break\n",
    "\n",
    "        if check_game_finished(game):\n",
    "            max_value = max_cell_value(game)\n",
    "            board_value = total_board_value(game)\n",
    "            trajectory.metrics[\"max_value\"] = max_value\n",
    "            trajectory.metrics[\"board_value\"] = board_value\n",
    "\n",
    "            if max_value < WINNING_VALUE:\n",
    "                # scale max value logarithmically between 0 for 2 and 1 for WINNING_VALUE\n",
    "                max_value_reward = (math.log(max_value, 2) - 1) / (\n",
    "                    math.log(WINNING_VALUE, 2) - 1\n",
    "                )\n",
    "                # scale board value logarithmically between 0 for 2 * 16 and 1 for WINNING_VALUE * 16\n",
    "                board_value_reward = (math.log(board_value, 2) - 1) / (\n",
    "                    math.log(WINNING_VALUE * 16, 2) - 1\n",
    "                )\n",
    "                # combine the two rewards, with max value having a higher weight\n",
    "                trajectory.reward = max_value_reward + (board_value_reward * 0.2)\n",
    "                trajectory.metrics[\"win\"] = 0\n",
    "            else:\n",
    "                # double reward if the agent wins\n",
    "                trajectory.reward = 2\n",
    "                trajectory.metrics[\"win\"] = 1\n",
    "            break\n",
    "\n",
    "    try:\n",
    "        if op_client.api_key:\n",
    "            await op_client.update_log_metadata(\n",
    "                filters=[\n",
    "                    {\n",
    "                        \"field\": \"completionId\",\n",
    "                        \"equals\": last_completion.id,\n",
    "                    }\n",
    "                ],\n",
    "                metadata={\n",
    "                    \"reward\": str(trajectory.reward),\n",
    "                    \"reward_assigned\": \"true\",\n",
    "                },\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REGISTER_TRAINABLE_MODEL:\n",
    "    for i in range(await model.get_step(), 50):\n",
    "        train_groups = await art.gather_trajectory_groups(\n",
    "            (\n",
    "                art.TrajectoryGroup(\n",
    "                    rollout(model, i, is_validation=False) for _ in range(4)\n",
    "                )\n",
    "                for _ in range(1)\n",
    "            ),\n",
    "            pbar_desc=\"gather\",\n",
    "            max_exceptions=1,\n",
    "        )\n",
    "        await model.delete_checkpoints()\n",
    "        await model.train(\n",
    "            train_groups,\n",
    "            config=art.TrainConfig(learning_rate=3e-5),\n",
    "            # Lowering the logprob_calculation_chunk_size is a memory saving measure\n",
    "            # to allow longer sequences (up to 4096 tokens) to be processed on a T4.\n",
    "            _config={\"logprob_calculation_chunk_size\": 8},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96a159d6504481dac704bcbcd02eb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather gpt-4o-mini:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0f0d2a3d2d4d9c86592b4db3055fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather gpt-4o:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9856771ed24bb6916c1b26aecbd1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather gpt-4.1:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-11' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-12' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-13' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-14' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-15' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-16' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-17' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-18' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-19' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-20' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-35' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-36' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-37' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-38' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-39' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-40' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-41' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-42' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-43' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-44' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-23' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-24' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-25' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-26' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-27' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-28' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-29' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-30' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-31' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-32' coro=<rollout() done, defined at /root/sky_workdir/src/art/utils/retry.py:74> exception=ValueError('In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.10/asyncio/tasks.py\", line 232, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/root/sky_workdir/src/art/utils/retry.py\", line 81, in async_wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 50, in rollout\n",
      "    raise e\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 44, in rollout\n",
      "    chat_completion = await get_completion()\n",
      "  File \"/tmp/ipykernel_11267/3596610620.py\", line 36, in get_completion\n",
      "    client = model.openai_client()\n",
      "  File \"/root/sky_workdir/src/art/model.py\", line 102, in openai_client\n",
      "    raise ValueError(\n",
      "ValueError: In order to create an OpenAI client you must provide an `inference_api_key` and `inference_base_url`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "\n",
    "\n",
    "async def log_comparison_model(comparison_model: art.Model):\n",
    "    trajectories = await art.gather_trajectory_groups(\n",
    "        (\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(comparison_model, 0, is_validation=True) for _ in range(12)\n",
    "            )\n",
    "            for _ in range(1)\n",
    "        ),\n",
    "        pbar_desc=f\"gather {comparison_model.name}\",\n",
    "        max_exceptions=1,\n",
    "    )\n",
    "\n",
    "    await comparison_model.log(\n",
    "        trajectories,\n",
    "        split=\"val\",\n",
    "    )\n",
    "\n",
    "gpt_4o_mini = art.Model(\n",
    "    name=\"gpt-4o-mini\",\n",
    "    project=\"2048-dev\",\n",
    "    inference_model_name=\"gpt-4o-mini\",\n",
    "    inference_base_url=\"https://api.openai.com/v1\",\n",
    "    inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "await gpt_4o_mini.register(api)\n",
    "\n",
    "gpt_4o = art.Model(\n",
    "    name=\"gpt-4o\",\n",
    "    project=\"2048-dev\",\n",
    "    inference_model_name=\"gpt-4o\",\n",
    "    inference_base_url=\"https://api.openai.com/v1\",\n",
    "    inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "await gpt_4o.register(api)\n",
    "\n",
    "gpt_4_1 = art.Model(\n",
    "    name=\"gpt-4.1\",\n",
    "    project=\"2048-dev\",\n",
    "    inference_model_name=\"gpt-4.1\",\n",
    "    inference_base_url=\"https://api.openai.com/v1\",\n",
    "    inference_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "await gpt_4_1.register(api)\n",
    "\n",
    "\n",
    "# Optional logging client\n",
    "op_client = AsyncOpenPipe()\n",
    "\n",
    "promises = []\n",
    "\n",
    "for comparison_model in [gpt_4o_mini, gpt_4o, gpt_4_1]:\n",
    "    promises.append(log_comparison_model(comparison_model))\n",
    "\n",
    "await asyncio.gather(*promises)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './.art/2048-dev/models/009/history.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mold_benchmarking\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerate_comparison_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_comparison_table\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mart\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mold_benchmarking\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BenchmarkedModelKey\n\u001b[0;32m----> 4\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_comparison_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2048-dev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBenchmarkedModelKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m009\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBenchmarkedModelKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBenchmarkedModelKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mboard_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(table\u001b[38;5;241m.\u001b[39mto_markdown())\n",
      "File \u001b[0;32m~/sky_workdir/src/art/utils/old_benchmarking/generate_comparison_table.py:12\u001b[0m, in \u001b[0;36mgenerate_comparison_table\u001b[0;34m(project, benchmark_keys, metrics, api_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_comparison_table\u001b[39m(\n\u001b[1;32m      7\u001b[0m     project: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      8\u001b[0m     benchmark_keys: \u001b[38;5;28mlist\u001b[39m[BenchmarkedModelKey],\n\u001b[1;32m      9\u001b[0m     metrics: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m     api_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./.art\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m---> 12\u001b[0m     benchmarked_models \u001b[38;5;241m=\u001b[39m \u001b[43mload_benchmarked_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m benchmarked_model \u001b[38;5;129;01min\u001b[39;00m benchmarked_models:\n",
      "File \u001b[0;32m~/sky_workdir/src/art/utils/old_benchmarking/load_benchmarked_models.py:37\u001b[0m, in \u001b[0;36mload_benchmarked_models\u001b[0;34m(project, benchmark_keys, metrics, api_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m split_dir \u001b[38;5;241m=\u001b[39m get_trajectories_split_dir(model_output_dir, benchmark_key\u001b[38;5;241m.\u001b[39msplit)\n\u001b[1;32m     35\u001b[0m history_logs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory.jsonl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# only include logs that have a recorded_at value\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         log \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './.art/2048-dev/models/009/history.jsonl'"
     ]
    }
   ],
   "source": [
    "from art.utils.old_benchmarking.generate_comparison_table import generate_comparison_table\n",
    "from art.utils.old_benchmarking.types import BenchmarkedModelKey\n",
    "\n",
    "table = generate_comparison_table(\n",
    "    project=\"2048-dev\",\n",
    "    benchmark_keys=[\n",
    "        BenchmarkedModelKey(\"009\", \"train\", [0, -2, -1]),\n",
    "        BenchmarkedModelKey(\"gpt-4o-mini\", \"val\"),\n",
    "        BenchmarkedModelKey(\"gpt-4o\", \"val\"),\n",
    "    ],\n",
    "    metrics=[\"reward\", \"max_value\", \"board_value\"],\n",
    ")\n",
    "\n",
    "print(table.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style='display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px;'>\n",
       "    <img src='./.art/2048-dev/benchmarks/2025-04-17_22-06-13_reward_line_graph.png' style='max-width: 100%; height: auto;'><img src='./.art/2048-dev/benchmarks/2025-04-17_22-06-13_max_value_line_graph.png' style='max-width: 100%; height: auto;'><img src='./.art/2048-dev/benchmarks/2025-04-17_22-06-13_board_value_line_graph.png' style='max-width: 100%; height: auto;'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from art.utils.old_benchmarking.generate_line_graphs import generate_line_graphs\n",
    "from art.utils.old_benchmarking.display_image_grid import display_image_grid\n",
    "from art.utils.old_benchmarking.types import BenchmarkedModelKey\n",
    "\n",
    "graph_image_paths = generate_line_graphs(\n",
    "    project=\"2048-dev\",\n",
    "    line_graph_keys=[\n",
    "        BenchmarkedModelKey(\"009\", \"train\"),\n",
    "        BenchmarkedModelKey(\"010\", \"train\"),\n",
    "        BenchmarkedModelKey(\"011\", \"train\"),\n",
    "    ],\n",
    "    comparison_keys=[\n",
    "        BenchmarkedModelKey(\"gpt-4o-mini\", \"val\"),\n",
    "        BenchmarkedModelKey(\"gpt-4o\", \"val\"),\n",
    "    ],\n",
    "    metrics=[\"reward\", \"max_value\", \"board_value\"],\n",
    ")\n",
    "\n",
    "display_image_grid(graph_image_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
