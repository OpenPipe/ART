{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train this agent, click *Runtime* and press *Run all*. Make sure you've enabled a free Tesla T4 GPU!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.com/invite/dnseNZuQ\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://openpipe.ai/blog/art-trainer-a-new-rl-trainer-for-agents\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Launch_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 3B model to play 2048. It will demonstrate how to set up a multi-turn agent, how to train it, and how to evaluate it.\n",
        "\n",
        "Completions will be logged to OpenPipe, and metrics will be logged to Weights & Biases.\n",
        "\n",
        " \n",
        "You will learn how to construct an [agentic environment](#Environment), how to define a [rollout](#Rollout), and how to run a [training loop](#Loop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy<2.0.0 in /root/miniconda3/lib/python3.10/site-packages (1.26.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install \"numpy<2.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WARNING:\n",
        "If you are running in Google Colab and installing numpy does not say \"Requirement already satisfied: numpy<2.0.0\" then click \"Runtime\" and \"Restart Session.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy version is 1.*.*, you're good to go!\n"
          ]
        }
      ],
      "source": [
        "# make sure we're using numpy 1.*.*\n",
        "import numpy as np\n",
        "\n",
        "if (np.__version__).startswith(\"1.\"):\n",
        "    print(\"Numpy version is 1.*.*, you're good to go!\")\n",
        "else:\n",
        "    raise ValueError(\"Please restart your runtime using the above instructions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Environment Variables\n",
        "\n",
        "Later on in the notebook, we'll be creating a model that automatically logs metrics to Weights & Biases. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n",
        "\n",
        "You can also optionally initiate an OpenPipe client to report completions to a [dashboard](https://app.openpipe.ai) to get a feel for what the completions your model is generating look like, and how they change over time. Logging to OpenPipe is free, but is not required for training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "# Required\n",
        "WANDB_API_KEY=\"\"\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
        "\n",
        "# Optional\n",
        "OPENPIPE_API_KEY=\"\"\n",
        "if OPENPIPE_API_KEY:\n",
        "    os.environ[\"OPENPIPE_API_KEY\"] = OPENPIPE_API_KEY\n",
        "\n",
        "# Make sure a WANDB_API_KEY is set\n",
        "assert os.getenv(\"WANDB_API_KEY\") is not None, \"Please set a WANDB_API_KEY environment variable either here, in your .env file, or in your Colab environment variables\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!uv pip install openpipe-art openpipe --prerelease allow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "environment"
        ]
      },
      "source": [
        "### Agentic Environment\n",
        "<a name=\"Environment\"></a>\n",
        "\n",
        "ART allows your agent to learn by interacting with its environment. In this example, we'll create an environment in which the agent can play 2048.\n",
        "\n",
        "Feel free to read as much or as little of this section's code as you'd like. The important thing to understand is that we're defining the rules of this agent's environment. In many cases, this will already be defined by the task you're trying to solve, but if you need to define a custom environment, this is how you do it.\n",
        "\n",
        "NOTE: To avoid OOM errors on a T4, we're reducing the winning value from 2048 to 256, which in turn reduces the minimum number of moves to win from 939 to 117."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import random\n",
        "from typing import TypedDict\n",
        "from typing import Literal\n",
        "import string\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "WINNING_VALUE = 256\n",
        "\n",
        "# Class that keeps track of state for a single game of 2048\n",
        "class TwentyFortyEightGame(TypedDict):\n",
        "    id: str\n",
        "    board: list[list[int | None]]\n",
        "\n",
        "# Randomly populates a cell on the board with a 2 or 4\n",
        "def populate_random_cell(game: TwentyFortyEightGame) -> None:\n",
        "    all_clear_coordinates = [\n",
        "        (i, j)\n",
        "        for i in range(len(game[\"board\"]))\n",
        "        for j in range(len(game[\"board\"][i]))\n",
        "        if game[\"board\"][i][j] is None\n",
        "    ]\n",
        "    random_clear_coordinates = random.choice(all_clear_coordinates)\n",
        "    # 90% chance to populate a 2, 10% chance to populate a 4\n",
        "    game[\"board\"][random_clear_coordinates[0]][random_clear_coordinates[1]] = (\n",
        "        2 if random.random() < 0.9 else 4\n",
        "    )\n",
        "\n",
        "# Generates a new game of 2048\n",
        "def generate_game(board_length: int = 4) -> TwentyFortyEightGame:\n",
        "    # random 6 character string\n",
        "    id = \"\".join(random.choices(string.ascii_letters + string.digits, k=6))\n",
        "    game = {\n",
        "        \"id\": id,\n",
        "        \"board\": [[None for _ in range(board_length)] for _ in range(board_length)],\n",
        "    }\n",
        "\n",
        "    # populate two random cells\n",
        "    populate_random_cell(game)\n",
        "    populate_random_cell(game)\n",
        "\n",
        "    return game\n",
        "\n",
        "# Renders the board in a human-readable format\n",
        "def render_board(game: TwentyFortyEightGame) -> str:\n",
        "    board = game[\"board\"]\n",
        "    # print something like this:\n",
        "    # _    | 2    | _    | 4\n",
        "    # 4    | 8    | 2    | 16\n",
        "    # 16   | 32   | 64   | 128\n",
        "    # _    | 2    | 2    | 4\n",
        "    # where _ is an empty cell\n",
        "\n",
        "    max_cell_width = max(\n",
        "        [len(str(cell)) for row in board for cell in row if cell is not None]\n",
        "    )\n",
        "\n",
        "    board_str = \"\"\n",
        "    for row in board:\n",
        "        # pad the cells with spaces to make them the same width\n",
        "        board_str += \"|\".join(\n",
        "            [\n",
        "                str(cell).rjust(max_cell_width)\n",
        "                if cell is not None\n",
        "                else \"_\".rjust(max_cell_width)\n",
        "                for cell in row\n",
        "            ]\n",
        "        )\n",
        "        board_str += \"\\n\"\n",
        "    return board_str\n",
        "\n",
        "\n",
        "# condense, privileging matches at the start of the sequence\n",
        "# sequences should be passed starting with cells that are the furthest in the direction in which the board is being condensed\n",
        "def condense_sequence(sequence: list[int | None]) -> list[int | None]:\n",
        "    condensed_sequence = []\n",
        "\n",
        "    gapless_sequence = [cell for cell in sequence if cell is not None]\n",
        "\n",
        "    i = 0\n",
        "    while i < len(gapless_sequence):\n",
        "        if (\n",
        "            i + 1 < len(gapless_sequence)\n",
        "            and gapless_sequence[i] == gapless_sequence[i + 1]\n",
        "        ):\n",
        "            condensed_sequence.append(gapless_sequence[i] * 2)\n",
        "            i += 2\n",
        "        else:\n",
        "            condensed_sequence.append(gapless_sequence[i])\n",
        "            i += 1\n",
        "\n",
        "    # pad the sequence with None at the end\n",
        "    return condensed_sequence + [None] * (4 - len(condensed_sequence))\n",
        "\n",
        "# Condenses the board in a given direction\n",
        "def condense_board(\n",
        "    game: TwentyFortyEightGame, direction: Literal[\"left\", \"right\", \"up\", \"down\"]\n",
        ") -> None:\n",
        "    if direction == \"left\":\n",
        "        for row in game[\"board\"]:\n",
        "            condensed_row = condense_sequence(row)\n",
        "            for i in range(len(row)):\n",
        "                row[i] = condensed_row[i]\n",
        "\n",
        "    if direction == \"right\":\n",
        "        for row in game[\"board\"]:\n",
        "            reversed_row = row[::-1]\n",
        "            # reverse the row before and after condensing\n",
        "            condensed_row = condense_sequence(reversed_row)[::-1]\n",
        "            for i in range(len(row)):\n",
        "                row[i] = condensed_row[i]\n",
        "\n",
        "    if direction == \"up\":\n",
        "        for col_index in range(len(game[\"board\"][0])):\n",
        "            column = [row[col_index] for row in game[\"board\"]]\n",
        "\n",
        "            condensed_column = condense_sequence(column)\n",
        "            for row_index in range(len(column)):\n",
        "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
        "\n",
        "    if direction == \"down\":\n",
        "        for col_index in range(len(game[\"board\"][0])):\n",
        "            column = [row[col_index] for row in game[\"board\"]]\n",
        "            reversed_column = column[::-1]\n",
        "            condensed_column = condense_sequence(reversed_column)[::-1]\n",
        "            for row_index in range(len(column)):\n",
        "                game[\"board\"][row_index][col_index] = condensed_column[row_index]\n",
        "\n",
        "\n",
        "# Applies an agent move to the game board\n",
        "def apply_agent_move(game: TwentyFortyEightGame, move_xml: str) -> None:\n",
        "    direction = None\n",
        "    # parse the move\n",
        "    try:\n",
        "        root = ET.fromstring(move_xml)\n",
        "        direction = root.text\n",
        "    except Exception as e:\n",
        "        raise ValueError(\"Invalid xml\")\n",
        "\n",
        "    if direction not in [\"left\", \"right\", \"up\", \"down\"]:\n",
        "        raise ValueError(\"Invalid direction\")\n",
        "\n",
        "    condense_board(game, direction)\n",
        "\n",
        "    populate_random_cell(game)\n",
        "\n",
        "# Returns the maximum cell value on the board\n",
        "def max_cell_value(game: TwentyFortyEightGame) -> int:\n",
        "    return max([cell for row in game[\"board\"] for cell in row if cell is not None])\n",
        "\n",
        "# Returns True if the game is finished\n",
        "def check_game_finished(game: TwentyFortyEightGame) -> bool:\n",
        "    if max_cell_value(game) >= WINNING_VALUE:\n",
        "        return True\n",
        "\n",
        "    # check if any cell is empty\n",
        "    if any(cell is None for row in game[\"board\"] for cell in row):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Returns the sum of all the cell values on the board\n",
        "def total_board_value(game: TwentyFortyEightGame) -> int:\n",
        "    return sum([cell for row in game[\"board\"] for cell in row if cell is not None])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now that we've defined the rules of our environment, we can create a model that will learn to play 2048. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openpipe.client import OpenPipe\n",
        "import random\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Uncomment this line and set your WANDB_API_KEY environment variable\n",
        "# or add it to your .env file or Colab environment variables\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\n",
        "assert (\n",
        "    os.getenv(\"WANDB_API_KEY\") is not None\n",
        "    and os.getenv(\"WANDB_API_KEY\") != \"YOUR_WANDB_API_KEY\"\n",
        "), \"You need to set your WANDB_API_KEY environment variable either here, in your .env file, or in your Colab environment variables\"\n",
        "\n",
        "# Initialize the server\n",
        "api = art.LocalAPI(\n",
        "    # Normally we don't want to run the server in-process, but for the output\n",
        "    # to show up properly on Google Colab we'll enable this.\n",
        "    in_process=True\n",
        ")\n",
        "\n",
        "# Declare the model\n",
        "model = await api.get_or_create_model(\n",
        "    name=\"004\",\n",
        "    project=\"2048-multi-turn\",\n",
        "    base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    # To run on a T4, we need to override some config defaults.\n",
        "    _config=art.config.ModelConfig(\n",
        "        init_args=art.config.InitArgs(\n",
        "            max_seq_length=4096,\n",
        "            enforce_eager=True,\n",
        "            enable_sleep_mode=False,\n",
        "        ),\n",
        "        engine_args=art.config.EngineArgs(\n",
        "            gpu_memory_utilization=0.57,\n",
        "            num_scheduler_steps=1,\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Optional logging client\n",
        "op_client = OpenPipe(\n",
        "    api_key=os.getenv(\"OPENPIPE_API_KEY\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "rollout"
        ]
      },
      "source": [
        "### Defining a Rollout\n",
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "A rollout is a single episode of an agent performing its task. It is generates one or more trajectories, which are lists of messages and choices.\n",
        "\n",
        "In this example, the rollout function generates a game of 2048, and the agent plays it until the game is finished. It then returns a trajectory which contains all the `system` and `user` messages presented to the agent, as well as all the `choices` that the agent made.\n",
        "\n",
        "When the game is finished the `reward` for the agent's performance is calculated based on the highest cell value on the board, which is then assigned to the trajectory.\n",
        "\n",
        "This rollout function will be called many times in parallel during each iteration of the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "from art.utils.get_trajectory_messages import get_trajectory_messages\n",
        "import openai\n",
        "import time\n",
        "import math\n",
        "import requests\n",
        "\n",
        "\n",
        "@art.retry(exceptions=(openai.LengthFinishReasonError, requests.ReadTimeout))\n",
        "async def rollout(\n",
        "    client: openai.AsyncOpenAI, iteration: int, is_validation: bool\n",
        ") -> art.Trajectory:\n",
        "\n",
        "    game = generate_game()\n",
        "\n",
        "    move_number = 0\n",
        "\n",
        "    trajectory = art.Trajectory(\n",
        "        messages_and_choices=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\",\n",
        "            }\n",
        "        ],\n",
        "        reward=0,\n",
        "        metrics={\"test\": 5},\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "\n",
        "        trajectory.messages_and_choices.append(\n",
        "            {\"role\": \"user\", \"content\": render_board(game)}\n",
        "        )\n",
        "\n",
        "        requested_at = int(time.time() * 1000)\n",
        "        messages = get_trajectory_messages(trajectory)\n",
        "\n",
        "        async def get_completion():\n",
        "            return await client.chat.completions.create(\n",
        "                max_completion_tokens=128,\n",
        "                messages=messages,\n",
        "                model=model.name,\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            chat_completion = await get_completion()\n",
        "            last_completion = chat_completion\n",
        "        except openai.LengthFinishReasonError as e:\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print(\"caught exception generating chat completion\", e)\n",
        "            raise e\n",
        "\n",
        "        try:\n",
        "            if op_client.api_key:\n",
        "                op_client.report(\n",
        "                    requested_at=requested_at,\n",
        "                    received_at=int(time.time() * 1000),\n",
        "                    req_payload={\n",
        "                        \"model\": model.name,\n",
        "                        \"messages\": messages,\n",
        "                        \"metadata\": {\n",
        "                            \"game_id\": game[\"id\"],\n",
        "                            \"notebook-id\": \"2048\",\n",
        "                            \"iteration\": str(iteration),\n",
        "                            \"validation\": str(is_validation),\n",
        "                            \"move_number\": str(move_number),\n",
        "                        },\n",
        "                    },\n",
        "                    resp_payload=chat_completion,\n",
        "                    status_code=200,\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Error reporting to OpenPipe: {e}\")\n",
        "\n",
        "        choice = chat_completion.choices[0]\n",
        "        content = choice.message.content\n",
        "        assert isinstance(content, str)\n",
        "        trajectory.messages_and_choices.append(choice)\n",
        "\n",
        "        try:\n",
        "            apply_agent_move(game, content)\n",
        "            move_number += 1\n",
        "        except ValueError:\n",
        "            trajectory.reward = -1\n",
        "            break\n",
        "\n",
        "        if check_game_finished(game):\n",
        "            max_value = max_cell_value(game)\n",
        "            board_value = total_board_value(game)\n",
        "            trajectory.metrics[\"max_value\"] = max_value\n",
        "            trajectory.metrics[\"board_value\"] = board_value\n",
        "\n",
        "            if max_value < WINNING_VALUE:\n",
        "                # scale max value logarithmically between 0 for 2 and 1 for WINNING_VALUE\n",
        "                max_value_reward = (math.log(max_value, 2) - 1) / (\n",
        "                    math.log(WINNING_VALUE, 2) - 1\n",
        "                )\n",
        "                # scale board value logarithmically between 0 for 2 * 16 and 1 for WINNING_VALUE * 16\n",
        "                board_value_reward = (math.log(board_value, 2) - 1) / (\n",
        "                    math.log(WINNING_VALUE * 16, 2) - 1\n",
        "                )\n",
        "                # combine the two rewards, with max value having a higher weight\n",
        "                trajectory.reward = max_value_reward + (board_value_reward * 0.2)\n",
        "            else:\n",
        "                # double reward if the agent wins\n",
        "                trajectory.reward = 2\n",
        "            break\n",
        "\n",
        "    try:\n",
        "        if op_client.api_key:\n",
        "            op_client.update_log_metadata(\n",
        "                filters=[\n",
        "                    {\n",
        "                        \"field\": \"completionId\",\n",
        "                        \"equals\": last_completion.id,\n",
        "                    }\n",
        "                ],\n",
        "                metadata={\n",
        "                    \"reward\": str(trajectory.reward),\n",
        "                    \"reward_assigned\": \"true\",\n",
        "                },\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating log metadata: {e}\")\n",
        "\n",
        "    return trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "loop"
        ]
      },
      "source": [
        "<a name=\"Loop\"></a>\n",
        "### Training Loop\n",
        "\n",
        "The training loop is where the magic happens. For each of the 50 iterations defined below, the rollout function will be called 18 times in parallel. This means that 18 games will be played at once. Each game will produce a trajectory, which will be used to update the model.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will delete all but the most recent checkpoint and train the model on the new trajectories.\n",
        "\n",
        "Inference will be blocked until the training is complete.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/sky_workdir/src/art/local/state.py:5: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  import unsloth  # type: ignore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 04-15 01:57:01 __init__.py:207] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 54.63%\n",
            "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.1 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 4096. Num Sequences = 226.\n",
            "Unsloth: vLLM's KV Cache can use up to 40.99 GB. Also swap space = 6 GB.\n",
            "INFO 04-15 01:57:12 config.py:549] This model supports multiple tasks: {'classify', 'score', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 04-15 01:57:12 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 04-15 01:57:12 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
            "INFO 04-15 01:57:12 cuda.py:229] Using Flash Attention backend.\n",
            "INFO 04-15 01:57:13 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 04-15 01:57:13 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 04-15 01:57:13 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79d0918ca4b24accb29eb7e7d51ef5b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0b299c458434b66a68c01c77d0191c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-15 01:57:15 model_runner.py:1115] Loading model weights took 2.2270 GB\n",
            "INFO 04-15 01:57:15 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 04-15 01:57:17 worker.py:267] Memory profiling takes 1.79 seconds\n",
            "INFO 04-15 01:57:17 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.57) = 45.09GiB\n",
            "INFO 04-15 01:57:17 worker.py:267] model weights take 2.23GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 1.26GiB; the rest of the memory reserved for KV Cache is 41.46GiB.\n",
            "INFO 04-15 01:57:18 executor_base.py:111] # cuda blocks: 75466, # CPU blocks: 10922\n",
            "INFO 04-15 01:57:18 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 294.79x\n",
            "INFO 04-15 01:57:26 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 10.88 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49a4b484badb4c96927cdcc57c6fb8d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4104 tokens (3976 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4110 tokens (3982 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4150 tokens (4022 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4107 tokens (3979 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4125 tokens (3997 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4151 tokens (4023 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4134 tokens (4006 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4139 tokens (4011 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4108 tokens (3980 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4107 tokens (3979 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
            "caught exception generating chat completion Error code: 400 - {'object': 'error', 'message': \"This model's maximum context length is 4096 tokens. However, you requested 4163 tokens (4035 in the messages, 128 in the completion). Please reduce the length of the messages or completion.\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marctic_fly\u001b[0m (\u001b[33mbased-op\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/root/sky_workdir/examples/2048/wandb/run-20250415_020201-004</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/based-op/2048-multi-turn/runs/004' target=\"_blank\">004</a></strong> to <a href='https://wandb.ai/based-op/2048-multi-turn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/based-op/2048-multi-turn' target=\"_blank\">https://wandb.ai/based-op/2048-multi-turn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/based-op/2048-multi-turn/runs/004' target=\"_blank\">https://wandb.ai/based-op/2048-multi-turn/runs/004</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No \"val/reward\" metric found in history\n",
            "Packed 7 trajectories into 3 sequences of length 4096\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "489cf661db2b47ee8a53150934e26d79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 10,000,000 | Num Epochs = 3 | Total steps = 30,000,000\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
            " \"-____-\"     Trainable parameters = 14,966,784/3,000,000,000 (0.50% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e46b4bfcc5814dc6b4afdcacf142c8df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "gather:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "openai_client = await model.openai_client()\n",
        "for i in range(await model.get_step(), 10):\n",
        "    train_groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(openai_client, i, is_validation=False) for _ in range(18)\n",
        "            )\n",
        "            for _ in range(1)\n",
        "        ),\n",
        "        pbar_desc=\"gather\",\n",
        "        max_exceptions=18,\n",
        "    )\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(train_groups, config=art.TrainConfig(learning_rate=3e-5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Model\n",
        "\n",
        "Just like that, you've trained an agent to play 2048! Now it's time to use your model outside of ART, in the wild! The easiest way to do that is to load it from disk, where it was saved after each training iteration, and either run inference on it locally or upload it to a central hub like HuggingFace.\n",
        "\n",
        "Check out the code below for small demo of the model you just trained playing 2048!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading model from .art/2048-multi-turn/models/003-3b/0063\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "move 10\n",
            "board:\n",
            "_|4|8|2\n",
            "_|_|_|8\n",
            "_|_|2|_\n",
            "_|_|_|_\n",
            "\n",
            "agent move: <move>down</move>\n",
            "updated board:\n",
            "_|_|_|_\n",
            "_|2|_|_\n",
            "_|_|8|2\n",
            "_|4|2|8\n",
            "\n",
            "\n",
            "move 20\n",
            "board:\n",
            " _| _| _| _\n",
            " _| _| _| 4\n",
            " _| _| 4|16\n",
            " 2| 4| 2|16\n",
            "\n",
            "agent move: <move>up</move>\n",
            "updated board:\n",
            " 2| 4| 4| 4\n",
            " _| _| 2|32\n",
            " _| _| _| _\n",
            " _| _| 2| _\n",
            "\n",
            "\n",
            "move 30\n",
            "board:\n",
            " _| 2|16| 2\n",
            " _| 4| _| 4\n",
            " _| _| _| 8\n",
            " _| 2| _|32\n",
            "\n",
            "agent move: <move>down</move>\n",
            "updated board:\n",
            " 2| _| _| 2\n",
            " _| 2| _| 4\n",
            " _| 4| _| 8\n",
            " _| 2|16|32\n",
            "\n",
            "\n",
            "move 40\n",
            "board:\n",
            " _| _| _| 2\n",
            " _| _| 2| 4\n",
            " _| 2| 4|64\n",
            " _| 4| 8| 2\n",
            "\n",
            "agent move: <move>right</move>\n",
            "updated board:\n",
            " _| _| 2| 2\n",
            " _| _| 2| 4\n",
            " _| 2| 4|64\n",
            " _| 4| 8| 2\n",
            "\n",
            "\n",
            "move 50\n",
            "board:\n",
            " _| _| _| 4\n",
            " _| _| 2| 4\n",
            " 2| 4| 8|64\n",
            " 2| 8|16| 2\n",
            "\n",
            "agent move: <move>up</move>\n",
            "updated board:\n",
            " 4| 4| 2| 8\n",
            " _| 8| 8|64\n",
            " _| _|16| 2\n",
            " 2| _| _| _\n",
            "\n",
            "\n",
            "move 60\n",
            "board:\n",
            " 2| 2| 4| 2\n",
            " 8|16| 8| 8\n",
            " _| 4|16|64\n",
            " _| 2| _| 2\n",
            "\n",
            "agent move: <move>down</move>\n",
            "updated board:\n",
            " _| 2| 4| 2\n",
            " _|16| 4| 8\n",
            " 2| 4| 8|64\n",
            " 8| 2|16| 2\n",
            "\n",
            "\n",
            "move 70\n",
            "board:\n",
            " _| 2| _| 4\n",
            " 4| 8| 4| 8\n",
            " 8| 4|32|64\n",
            " 2| 2|16| 2\n",
            "\n",
            "agent move: <move>right</move>\n",
            "updated board:\n",
            " _| _| 2| 4\n",
            " 4| 8| 4| 8\n",
            " 8| 4|32|64\n",
            " 2| 4|16| 2\n",
            "\n",
            "\n",
            "move 80\n",
            "board:\n",
            " 4| 2|16| 8\n",
            " 4| 2|32|16\n",
            " 2| 4|16|64\n",
            " _| 8| 2| 4\n",
            "\n",
            "agent move: <move>up</move>\n",
            "updated board:\n",
            " 8| 4|16| 8\n",
            " 2| 4|32|16\n",
            " 2| 8|16|64\n",
            " _| _| 2| 4\n",
            "\n",
            "\n",
            "move 90\n",
            "board:\n",
            " 4| 4|16| 8\n",
            " 4|16|32|16\n",
            " _| 8|16|64\n",
            " 2| 2| 4| 8\n",
            "\n",
            "agent move: <move>down</move>\n",
            "updated board:\n",
            " _| 4|16| 8\n",
            " 2|16|32|16\n",
            " 8| 8|16|64\n",
            " 2| 2| 4| 8\n",
            "\n",
            "game finished in 96 moves\n",
            "game lost! ðŸ˜¢\n",
            "final board:\n",
            "\n",
            " 2| 4|16| 8\n",
            " 2| 2|64|16\n",
            " 4| 4|16|64\n",
            " 2| 2| 4| 8\n",
            "\n",
            "max value: 64\n",
            "board value: 218\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "# example: .art/2048-multi-turn/models/001/0003\n",
        "lora_model_path = f\".art/{model.project}/models/{model.name}/{await model.get_step():04d}\"\n",
        "\n",
        "print(f\"loading model from {lora_model_path}\\n\")\n",
        "\n",
        "peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = lora_model_path,\n",
        "    max_seq_length = 16384,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "FastLanguageModel.for_inference(peft_model)\n",
        "\n",
        "game = generate_game()\n",
        "move_number = 0\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an excellent 2048 player. Always choose the move most likely to lead to combine cells to eventually reach the number 2048. Optional moves are 'left', 'right', 'up', 'down'. Return your move as an XML object with a single property 'move', like so: <move>left</move>\"},\n",
        "]\n",
        "\n",
        "while not check_game_finished(game):\n",
        "    rendered_board = render_board(game)\n",
        "    messages.append({\"role\": \"user\", \"content\": rendered_board})\n",
        "\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "\n",
        "    content = \"\"\n",
        "\n",
        "    def get_completion() -> str:\n",
        "        with torch.no_grad():\n",
        "            outputs = peft_model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=100,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "            return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    try:\n",
        "        content = get_completion()\n",
        "    except Exception as e:\n",
        "        print(\"caught exception generating chat completion\", e)\n",
        "        raise e\n",
        "\n",
        "    messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "    \n",
        "\n",
        "    try:\n",
        "        apply_agent_move(game, content)\n",
        "        move_number += 1\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Invalid move on move {move_number}: {content}\")\n",
        "\n",
        "    # print the board every 10 moves\n",
        "    if (move_number % 10 == 0):\n",
        "        print(f\"\\nmove {move_number}\")\n",
        "        print(f\"board:\\n{rendered_board}\")\n",
        "        print(f\"agent move: {content}\")\n",
        "        print(f\"updated board:\\n{render_board(game)}\")\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "max_value = max_cell_value(game)\n",
        "board_value = total_board_value(game)\n",
        "\n",
        "print(f\"game finished in {move_number} moves\")\n",
        "\n",
        "if max_value >= WINNING_VALUE:\n",
        "    print(f\"game won! ðŸ’ª\")\n",
        "else:\n",
        "    print(f\"game lost! ðŸ˜¢\")\n",
        "\n",
        "print(f\"final board:\\n\\n{render_board(game)}\")\n",
        "\n",
        "print(f\"max value: {max_value}\")\n",
        "print(f\"board value: {board_value}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/notebooks/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.com/invite/dnseNZuQ\"><img src=\"https://github.com/openpipe/art/raw/notebooks/assets/Discord_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://openpipe.ai/blog/art-trainer-a-new-rl-trainer-for-agents\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Launch_pill.png\" height=\"50\"></a>\n",
        "\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
