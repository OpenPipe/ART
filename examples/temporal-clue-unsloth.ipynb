{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7438/3675116539.py:2: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:496: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  left = re.match(\"[\\s\\n]{4,}\", leftover).span()[1]\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:924: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  .replace(\"*\", \"\\*\").replace(\"^\", \"\\^\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:924: SyntaxWarning: invalid escape sequence '\\^'\n",
      "  .replace(\"*\", \"\\*\").replace(\"^\", \"\\^\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:925: SyntaxWarning: invalid escape sequence '\\-'\n",
      "  .replace(\"-\", \"\\-\").replace(\"_\", \"\\_\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:925: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  .replace(\"-\", \"\\-\").replace(\"_\", \"\\_\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:926: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  .replace(\":\", \"\\:\").replace(\"+\", \"\\+\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:926: SyntaxWarning: invalid escape sequence '\\+'\n",
      "  .replace(\":\", \"\\:\").replace(\"+\", \"\\+\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:927: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  .replace(\".\", \"\\.\").replace(\",\", \"\\,\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:927: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  .replace(\".\", \"\\.\").replace(\",\", \"\\,\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:928: SyntaxWarning: invalid escape sequence '\\('\n",
      "  .replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:928: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  .replace(\"(\", \"\\(\").replace(\")\", \"\\)\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:929: SyntaxWarning: invalid escape sequence '\\['\n",
      "  .replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:929: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  .replace(\"[\", \"\\[\").replace(\"]\", \"\\]\")\\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1018: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  if \"loss\\_function\" in cross_entropy_find and \"loss_function\" not in forward:\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1020: SyntaxWarning: invalid escape sequence '\\_'\n",
      "  elif \"loss\\_function\" not in cross_entropy_find and \"loss_function\" in forward:\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1056: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  if \"logits = outputs\\.logits\" in cross_entropy_find:\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1163: SyntaxWarning: invalid escape sequence '\\:'\n",
      "  r\"for ([^\\s]{1,}) in \" + modulelist_item + \"\\:[\\n]\" + \\\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1406: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_find = f\"{call_class}\\(([^\\)]{{1,}})\\)\"\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1406: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  regex_find = f\"{call_class}\\(([^\\)]{{1,}})\\)\"\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1412: SyntaxWarning: invalid escape sequence '\\('\n",
      "  regex_find = \"def forward\\(([^\\)]{1,})\\)\"\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1518: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  inherited_modules = re.findall(r\"class ([^\\s]{1,})\\(\" + inherited_class + \"\\)\", full_source)\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/compiler.py:1572: SyntaxWarning: invalid escape sequence '\\('\n",
      "  called = re.findall(r\"[\\s]{1,}\" + re.escape(function) + \"\\(.+?\\)\", full_source, flags = re.DOTALL)\n",
      "/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/unsloth_zoo/peft_utils.py:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  name = re.sub(\"\\.([\\d]{1,})\\.\", r\"[\\1].\", name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-04 22:13:55 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit with actual GPU utilization = 79.39%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 8192. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 52.15 GB. Also swap space = 6 GB.\n",
      "INFO 04-04 22:14:17 config.py:549] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.5.mlp', 'model.layers.24.mlp', 'model.layers.44.mlp', 'model.layers.46.mlp', 'model.layers.23.self_attn'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-04 22:14:17 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":368}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcf0ae3c0a747a58662e3cfda8b6076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8d346338c44092a5caf6ec2080b1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12ebbc6273b430386b6d539aac4cf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f28b4455374107a6bdd73252a44339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c376a8b686714316acd1b596af871c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c838e246fa94bd5a8fb7d1fbba2433e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09295e2a095b4c6caa2f70adca5e91e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 22:14:20 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 04-04 22:14:20 registry.py:335] `mm_limits` has already been set for model=unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit, and will be overwritten by the new values.\n",
      "INFO 04-04 22:14:20 model_runner.py:1110] Starting to load model unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit...\n",
      "INFO 04-04 22:14:20 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 04-04 22:14:21 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942179d62b3149f588e7afa2a8080d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c319baf0e5264841a04d928a1515b2fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f23d75334144fb2b6b93d5eece161ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 22:14:48 weight_utils.py:270] Time spent downloading weights for unsloth/qwen2.5-14b-instruct-unsloth-bnb-4bit: 27.014179 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef642148fb42467a9a7dd90bc1a43c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35f218eeb71410ab28e28c363b13345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 22:14:52 model_runner.py:1115] Loading model weights took 10.6011 GB\n",
      "INFO 04-04 22:14:52 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-04 22:14:59 worker.py:267] Memory profiling takes 6.95 seconds\n",
      "INFO 04-04 22:14:59 worker.py:267] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.79) = 62.81GiB\n",
      "INFO 04-04 22:14:59 worker.py:267] model weights take 10.60GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 2.09GiB; the rest of the memory reserved for KV Cache is 49.96GiB.\n",
      "INFO 04-04 22:15:00 executor_base.py:111] # cuda blocks: 17054, # CPU blocks: 2048\n",
      "INFO 04-04 22:15:00 executor_base.py:116] Maximum concurrency for 8192 tokens per request: 33.31x\n",
      "INFO 04-04 22:15:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:45<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-04 22:15:47 model_runner.py:1562] Graph capturing finished in 46 secs, took 9.48 GiB\n",
      "INFO 04-04 22:15:47 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 55.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.3.19 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bbb38a272d47bcb8cb438764608f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import peft\n",
    "import unsloth\n",
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import re\n",
    "from typing import TypedDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TemporalCluePuzzle(TypedDict):\n",
    "    num_clues: int\n",
    "    prompt: str\n",
    "    solution: dict[str, str]\n",
    "\n",
    "\n",
    "puzzles: list[TemporalCluePuzzle] = json.load(open(\"./data/temporal-clue/puzzles.json\"))\n",
    "val_puzzles = puzzles[:64]\n",
    "test_puzzles = puzzles[64:128]\n",
    "train_puzzles = puzzles[128:]\n",
    "random.seed(42)\n",
    "random.shuffle(train_puzzles)\n",
    "\n",
    "\n",
    "api = art.UnslothAPI(in_process=True, wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api._get_or_create_model(\n",
    "    name=\"temporal-clue-unsloth-002\",\n",
    "    base_model=\"Qwen/Qwen2.5-14B-Instruct\",\n",
    "    _config={\n",
    "        \"init_args\": {\n",
    "            \"enable_sleep_mode\": True,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, puzzle: TemporalCluePuzzle\n",
    ") -> art.Trajectory:\n",
    "    messages: art.Messages = [{\"role\": \"user\", \"content\": puzzle[\"prompt\"]}]\n",
    "    chat_completion = await client.chat.completions.create(\n",
    "        messages=messages, model=model.name\n",
    "    )\n",
    "    choice = chat_completion.choices[0]\n",
    "    content = choice.message.content\n",
    "    assert isinstance(content, str)\n",
    "    num_correct = 0\n",
    "    for key, value in puzzle[\"solution\"].items():\n",
    "        if matches := re.findall(rf\"{key}\\. ([A-Za-z \\.:-]+)\", content):\n",
    "            match = matches[-1]\n",
    "            if match.strip().lower() == value.lower():\n",
    "                num_correct += 1\n",
    "    reward = acc = num_correct / len(puzzle[\"solution\"])\n",
    "    return art.Trajectory(\n",
    "        messages_and_choices=[*messages, choice], reward=reward, metrics={\"acc\": acc}\n",
    "    )\n",
    "\n",
    "\n",
    "stride = 16\n",
    "openai_client = await model.openai_client()\n",
    "\n",
    "gather_task = asyncio.create_task(\n",
    "    art.gather_trajectories(\n",
    "        (\n",
    "            (rollout(openai_client, puzzle) for _ in range(50))\n",
    "            for puzzle in train_puzzles[:stride]\n",
    "        ),\n",
    "        pbar_desc=\"train\",\n",
    "        return_exceptions=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# for i in range(await model.get_iteration(), 1_000):\n",
    "#     val_groups, train_groups = await asyncio.gather(\n",
    "#         art.gather_trajectories(\n",
    "#             (\n",
    "#                 (rollout(openai_client, puzzle) for _ in range(2))\n",
    "#                 for puzzle in val_puzzles\n",
    "#             ),\n",
    "#             pbar_desc=\"val\",\n",
    "#             stream_chat_completions=8,\n",
    "#             return_exceptions=False,\n",
    "#         ),\n",
    "#         art.gather_trajectories(\n",
    "#             (\n",
    "#                 (rollout(openai_client, puzzle) for _ in range(50))\n",
    "#                 for puzzle in train_puzzles[i * stride : (i + 1) * stride]\n",
    "#             ),\n",
    "#             pbar_desc=\"train\",\n",
    "#             return_exceptions=False,\n",
    "#         ),\n",
    "#     )\n",
    "#     await model.log(val_groups)\n",
    "#     await model.clear_iterations()\n",
    "#     await model.tune(\n",
    "#         train_groups,\n",
    "#         config=art.TuneConfig(\n",
    "#             lr=5e-5, sequence_length=8192, plot_tensors=True, verbosity=2\n",
    "#         ),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mode\n"
     ]
    }
   ],
   "source": [
    "async with api._services[\"temporal-clue-unsloth-002\"].state.vllm.train_mode():\n",
    "    print(\"train mode\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
