{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".cell-output-ipywidget-background {\n",
              "    background-color: transparent !important;\n",
              "}\n",
              ":root {\n",
              "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
              "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
              "}  \n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        ".cell-output-ipywidget-background {\n",
        "    background-color: transparent !important;\n",
        "}\n",
        ":root {\n",
        "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
        "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
        "}  \n",
        "</style>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenPipe client initialized\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  import unsloth  # type: ignore\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 04-18 22:26:30 __init__.py:207] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
            "   \\\\   /|    NVIDIA H100 PCIe. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.47%\n",
            "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.1 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
            "Unsloth: vLLM's KV Cache can use up to 59.84 GB. Also swap space = 6 GB.\n",
            "INFO 04-18 22:26:46 config.py:549] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.30.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 04-18 22:26:46 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":368}, use_cached_outputs=False, \n",
            "INFO 04-18 22:26:47 cuda.py:229] Using Flash Attention backend.\n",
            "WARNING 04-18 22:26:47 registry.py:335] `mm_limits` has already been set for model=unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit, and will be overwritten by the new values.\n",
            "INFO 04-18 22:26:47 model_runner.py:1110] Starting to load model unsloth/qwen2.5-3b-instruct-unsloth-bnb-4bit...\n",
            "INFO 04-18 22:26:47 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 04-18 22:26:48 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.83it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.83it/s]\n",
            "\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.54it/s]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-18 22:26:50 model_runner.py:1115] Loading model weights took 2.2265 GB\n",
            "INFO 04-18 22:26:50 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 04-18 22:26:53 worker.py:267] Memory profiling takes 2.93 seconds\n",
            "INFO 04-18 22:26:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.78) = 62.06GiB\n",
            "INFO 04-18 22:26:53 worker.py:267] model weights take 2.23GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 2.71GiB; the rest of the memory reserved for KV Cache is 56.98GiB.\n",
            "INFO 04-18 22:26:53 executor_base.py:111] # cuda blocks: 103733, # CPU blocks: 10922\n",
            "INFO 04-18 22:26:53 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 50.65x\n",
            "INFO 04-18 22:26:58 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:43<00:00,  1.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-18 22:27:42 model_runner.py:1562] Graph capturing finished in 44 secs, took 1.30 GiB\n",
            "INFO 04-18 22:27:42 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 52.33 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n",
            "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
          ]
        }
      ],
      "source": [
        "import art\n",
        "from dotenv import load_dotenv\n",
        "import random\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from openpipe.client import OpenPipe\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "op_client = OpenPipe()\n",
        "print(\"OpenPipe client initialized\")\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "api = art.LocalAPI()\n",
        "\n",
        "class CustomConfig(BaseModel):\n",
        "    litellm_model_name: str | None = None\n",
        "\n",
        "model = art.TrainableModel(\n",
        "    name=\"010\", project=\"tic-tac-toe\", base_model=\"Qwen/Qwen2.5-3B-Instruct\"\n",
        ")\n",
        "await model.register(api)\n",
        "\n",
        "gpt_4o_mini = art.Model(\n",
        "    name=\"gpt-4o-mini\",\n",
        "    project=\"tic-tac-toe\",\n",
        "    config=CustomConfig(\n",
        "        litellm_model_name=\"openai/gpt-4o-mini\",\n",
        "    ),\n",
        ")\n",
        "await gpt_4o_mini.register(api)\n",
        "\n",
        "gpt_4o = art.Model(\n",
        "    name=\"gpt-4o\",\n",
        "    project=\"tic-tac-toe\",\n",
        "    config=CustomConfig(\n",
        "        litellm_model_name=\"openai/gpt-4o\",\n",
        "    ),\n",
        ")\n",
        "await gpt_4o.register(api)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art\n",
        "import json\n",
        "import random\n",
        "from typing import TypedDict\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class TicTacToeGame(TypedDict):\n",
        "    board: list[list[str]]\n",
        "    agent_symbol: Literal[\"x\", \"o\"]\n",
        "    opponent_symbol: Literal[\"x\", \"o\"]\n",
        "\n",
        "\n",
        "def generate_game(board_length: int = 3) -> TicTacToeGame:\n",
        "    board = [[\"_\" for _ in range(board_length)] for _ in range(board_length)]\n",
        "    agent_symbol = random.choice([\"x\", \"o\"])\n",
        "    opponent_symbol = \"x\" if agent_symbol == \"o\" else \"o\"\n",
        "    return {\n",
        "        \"board\": board,\n",
        "        \"agent_symbol\": agent_symbol,\n",
        "        \"opponent_symbol\": opponent_symbol,\n",
        "    }\n",
        "\n",
        "\n",
        "def render_board(game: TicTacToeGame) -> str:\n",
        "    board = game[\"board\"]\n",
        "    board_length = len(board)\n",
        "    # print something like this:\n",
        "    #    1   2   3\n",
        "    # A  _ | x | x\n",
        "    # B  o | _ | _\n",
        "    # C  _ | o | _\n",
        "    # where _ is an empty cell\n",
        "\n",
        "    board_str = \"   \" + \"   \".join([str(i + 1) for i in range(board_length)]) + \"\\n\"\n",
        "    for i in range(board_length):\n",
        "        board_str += f\"{chr(65 + i)}  {board[i][0]} | {board[i][1]} | {board[i][2]}\\n\"\n",
        "    return board_str\n",
        "\n",
        "\n",
        "def get_opponent_move(game: TicTacToeGame) -> tuple[int, int]:\n",
        "    # get a random empty cell\n",
        "    empty_cells = [\n",
        "        (i, j) for i in range(3) for j in range(3) if game[\"board\"][i][j] == \"_\"\n",
        "    ]\n",
        "    return random.choice(empty_cells)\n",
        "\n",
        "\n",
        "class AgentMove(BaseModel):\n",
        "    reason: str\n",
        "    square: str\n",
        "\n",
        "\n",
        "def apply_agent_move(game: TicTacToeGame, move: str) -> None:\n",
        "    board_length = len(game[\"board\"])\n",
        "    json_move = json.loads(move)\n",
        "    square = json_move[\"square\"]\n",
        "\n",
        "    try:\n",
        "        row_index = ord(square[0]) - 65\n",
        "        col_index = int(square[1]) - 1\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        raise ValueError(\"Unable to parse square\")\n",
        "\n",
        "    if (\n",
        "        row_index < 0\n",
        "        or row_index >= board_length\n",
        "        or col_index < 0\n",
        "        or col_index >= board_length\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            f\"Invalid move, row or column out of bounds: {row_index}, {col_index}\"\n",
        "        )\n",
        "\n",
        "    # check if the move is valid\n",
        "    if game[\"board\"][row_index][col_index] != \"_\":\n",
        "        raise ValueError(\"Square already occupied\")\n",
        "\n",
        "    game[\"board\"][row_index][col_index] = game[\"agent_symbol\"]\n",
        "\n",
        "\n",
        "def check_winner(board: list[list[str]]) -> Literal[\"x\", \"o\", \"draw\", None]:\n",
        "    board_length = len(board)\n",
        "    # check rows\n",
        "    for row in board:\n",
        "        if row.count(row[0]) == board_length and row[0] != \"_\":\n",
        "            return row[0]\n",
        "    # check columns\n",
        "    for col in range(board_length):\n",
        "        if [board[row][col] for row in range(board_length)].count(\n",
        "            board[0][col]\n",
        "        ) == board_length and board[0][col] != \"_\":\n",
        "            return board[0][col]\n",
        "\n",
        "    # top right to bottom left\n",
        "    upward_diagonal = [board[i][board_length - i - 1] for i in range(board_length)]\n",
        "    if (\n",
        "        upward_diagonal.count(upward_diagonal[0]) == board_length\n",
        "        and upward_diagonal[0] != \"_\"\n",
        "    ):\n",
        "        return upward_diagonal[0]\n",
        "\n",
        "    # top left to bottom right\n",
        "    downward_diagonal = [board[i][i] for i in range(board_length)]\n",
        "    if (\n",
        "        downward_diagonal.count(downward_diagonal[0]) == board_length\n",
        "        and downward_diagonal[0] != \"_\"\n",
        "    ):\n",
        "        return downward_diagonal[0]\n",
        "\n",
        "    # check for draw\n",
        "    if all(cell != \"_\" for row in board for cell in row):\n",
        "        return \"draw\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_trajectory_messages(trajectory: art.Trajectory) -> art.Messages:\n",
        "    messages: art.Messages = []\n",
        "    for item in trajectory.messages_and_choices:\n",
        "        # if item is not a dict, convert it to a dict\n",
        "        if not isinstance(item, dict):\n",
        "            item = item.to_dict()\n",
        "\n",
        "        # check if item is a choice\n",
        "        if \"message\" in item:\n",
        "            messages.append(\n",
        "                {\"role\": \"assistant\", \"content\": item[\"message\"][\"content\"]}\n",
        "            )\n",
        "        else:\n",
        "            # otherwise it's a message\n",
        "            messages.append(item)\n",
        "    return messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import art\n",
        "import openai\n",
        "import time\n",
        "from litellm import acompletion\n",
        "\n",
        "from art.utils.litellm import convert_litellm_choice_to_openai\n",
        "\n",
        "\n",
        "@art.retry(exceptions=(openai.LengthFinishReasonError,))\n",
        "async def rollout(\n",
        "    model: art.Model, iteration: int, is_validation: bool\n",
        ") -> art.Trajectory:\n",
        "    game = generate_game()\n",
        "\n",
        "    trajectory = art.Trajectory(\n",
        "        messages_and_choices=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return the move in the format 'A1', 'B2', 'C3', etc. You are the {game['agent_symbol']} symbol.\",\n",
        "            }\n",
        "        ],\n",
        "        reward=0,\n",
        "        metrics={\"test\": 5},\n",
        "    )\n",
        "\n",
        "    if game[\"agent_symbol\"] == \"o\":\n",
        "        starting_opponent_move = get_opponent_move(game)\n",
        "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
        "            \"opponent_symbol\"\n",
        "        ]\n",
        "\n",
        "    while check_winner(game[\"board\"]) is None:\n",
        "        trajectory.messages_and_choices.append(\n",
        "            {\"role\": \"user\", \"content\": render_board(game)}\n",
        "        )\n",
        "\n",
        "        requested_at = int(time.time() * 1000)\n",
        "        messages = get_trajectory_messages(trajectory)\n",
        "\n",
        "            \n",
        "\n",
        "        try:\n",
        "            model_id = model.config.litellm_model_name if isinstance(model.config, CustomConfig) else f\"hosted_vllm/{model.name}\"\n",
        "            chat_completion = await acompletion(\n",
        "                base_url=model.base_url,\n",
        "                api_key=model.api_key,\n",
        "                model=model_id,\n",
        "                messages=messages,\n",
        "                max_completion_tokens=128,\n",
        "            )\n",
        "            last_completion = chat_completion\n",
        "        except openai.LengthFinishReasonError as e:\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print(\"caught exception generating chat completion\")\n",
        "            print(e)\n",
        "            global failing_trajectory\n",
        "            failing_trajectory = trajectory\n",
        "            raise e\n",
        "\n",
        "        try:\n",
        "            op_client.report(\n",
        "                requested_at=requested_at,\n",
        "                received_at=int(time.time() * 1000),\n",
        "                req_payload={\n",
        "                    \"model\": model.name,\n",
        "                    \"messages\": messages,\n",
        "                    \"metadata\": {\n",
        "                        \"notebook-id\": \"tic-tac-toe\",\n",
        "                        \"iteration\": str(iteration),\n",
        "                        \"validation\": str(is_validation),\n",
        "                        \"move_number\": str(len(trajectory.messages_and_choices) - 1),\n",
        "                    },\n",
        "                },\n",
        "                resp_payload=chat_completion,\n",
        "                status_code=200,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error reporting to OpenPipe: {e}\")\n",
        "\n",
        "        choice = convert_litellm_choice_to_openai(chat_completion.choices[0])\n",
        "        content = choice.message.content\n",
        "        assert isinstance(content, str)\n",
        "        trajectory.messages_and_choices.append(choice)\n",
        "\n",
        "        try:\n",
        "            apply_agent_move(game, content)\n",
        "        except ValueError as e:\n",
        "            trajectory.reward = -1\n",
        "            break\n",
        "\n",
        "        if check_winner(game[\"board\"]) is not None:\n",
        "            break\n",
        "\n",
        "        opponent_move = get_opponent_move(game)\n",
        "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
        "\n",
        "    winner = check_winner(game[\"board\"])\n",
        "\n",
        "    if winner == game[\"agent_symbol\"]:\n",
        "        trajectory.reward = 1\n",
        "    elif winner == game[\"opponent_symbol\"]:\n",
        "        trajectory.reward = 0\n",
        "    elif winner == \"draw\":\n",
        "        trajectory.reward = 0.5\n",
        "\n",
        "    try:\n",
        "        op_client.update_log_metadata(\n",
        "            filters=[\n",
        "                {\n",
        "                    \"field\": \"completionId\",\n",
        "                    \"equals\": last_completion.id,\n",
        "                }\n",
        "            ],\n",
        "            metadata={\n",
        "                \"reward\": str(trajectory.reward),\n",
        "                \"reward_assigned\": \"true\",\n",
        "            },\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating log metadata: {e}\")\n",
        "\n",
        "    return trajectory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(await model.get_step(), 10):\n",
        "    train_groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, i, is_validation=False) for _ in range(20)\n",
        "            )\n",
        "            for _ in range(1)\n",
        "        ),\n",
        "        pbar_desc=\"train\",\n",
        "    )\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(train_groups, config=art.TrainConfig(learning_rate=1e-4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "async def log_comparison_model(comparison_model: art.Model):\n",
        "    trajectories = await art.gather_trajectory_groups(\n",
        "            (\n",
        "                art.TrajectoryGroup(rollout(comparison_model, 0, is_validation=True) for _ in range(12))\n",
        "            for _ in range(1)\n",
        "        ),\n",
        "        pbar_desc=f\"gather {comparison_model.name}\",\n",
        "        max_exceptions=1,\n",
        "    )\n",
        "\n",
        "    await comparison_model.log(\n",
        "        trajectories,\n",
        "        split=\"val\",\n",
        "    )\n",
        "\n",
        "promises = []\n",
        "\n",
        "for comparison_model in [gpt_4o_mini, gpt_4o]:\n",
        "    promises.append(log_comparison_model(comparison_model))\n",
        "\n",
        "await asyncio.gather(*promises)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
