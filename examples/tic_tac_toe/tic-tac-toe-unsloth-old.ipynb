{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "from openpipe.client import OpenPipe\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "op_client = OpenPipe()\n",
    "print(\"OpenPipe client initialized\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "api = art.LocalAPI(wandb_project=\"agent-reinforcement-training\")\n",
    "model = await api.get_or_create_model(\n",
    "    name=\"tic-tac-toe-unsloth-003\",\n",
    "    base_model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    _config={\"init_args\": {\"num_scheduler_steps\": 1}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcpuser/sky_workdir/src/art/__init__.py:10: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-10 22:15:20 __init__.py:207] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Qwen2 patching. Transformers: 4.51.1. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit with actual GPU utilization = 78.33%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 79.11 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 368.\n",
      "Unsloth: vLLM's KV Cache can use up to 56.1 GB. Also swap space = 6 GB.\n",
      "INFO 04-10 22:15:30 config.py:549] This model supports multiple tasks: {'score', 'generate', 'classify', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.mlp', 'model.layers.3.mlp', 'model.layers.4.mlp', 'model.layers.25.mlp', 'model.layers.26.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 04-10 22:15:30 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":368}, use_cached_outputs=False, \n",
      "INFO 04-10 22:15:31 cuda.py:229] Using Flash Attention backend.\n",
      "WARNING 04-10 22:15:31 registry.py:335] `mm_limits` has already been set for model=unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit, and will be overwritten by the new values.\n",
      "INFO 04-10 22:15:31 model_runner.py:1110] Starting to load model unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit...\n",
      "INFO 04-10 22:15:31 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 04-10 22:15:32 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.71it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.81it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.67it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 22:15:34 model_runner.py:1115] Loading model weights took 6.6961 GB\n",
      "INFO 04-10 22:15:34 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-10 22:15:37 worker.py:267] Memory profiling takes 2.21 seconds\n",
      "INFO 04-10 22:15:37 worker.py:267] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.78) = 61.97GiB\n",
      "INFO 04-10 22:15:37 worker.py:267] model weights take 6.70GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 4.72GiB; the rest of the memory reserved for KV Cache is 50.40GiB.\n",
      "INFO 04-10 22:15:37 executor_base.py:111] # cuda blocks: 58980, # CPU blocks: 7021\n",
      "INFO 04-10 22:15:37 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 28.80x\n",
      "INFO 04-10 22:15:39 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:33<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-10 22:16:13 model_runner.py:1562] Graph capturing finished in 33 secs, took 5.56 GiB\n",
      "INFO 04-10 22:16:13 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 38.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ad224722ef487080f44e239635456c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:asyncio:Exception in callback _log_task_completion(error_callback=<bound method...75eba28cb860>>)(<Task finishe...ep decoding')>) at /home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py:48\n",
      "handle: <Handle _log_task_completion(error_callback=<bound method...75eba28cb860>>)(<Task finishe...ep decoding')>) at /home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py:48>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 58, in _log_task_completion\n",
      "    return_value = task.result()\n",
      "                   ^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 825, in run_engine_loop\n",
      "    result = task.result()\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 316, in __step_run_and_handle_result\n",
      "    result = coro.throw(exc)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/src/art/local/vllm.py\", line 130, in engine_step\n",
      "    return await _engine_step(virtual_engine)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 748, in engine_step\n",
      "    request_outputs = await self.engine.step_async(virtual_engine)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 353, in step_async\n",
      "    outputs = await self.model_executor.execute_model_async(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 250, in execute_model_async\n",
      "    output = await make_async(self.execute_model)(execute_model_req)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "    ^^^^^^^^^^\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n",
      "    future.result()\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py\", line 202, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 139, in execute_model\n",
      "    output = self.collective_rpc(\"execute_model\",\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/utils.py\", line 2196, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/worker_base.py\", line 420, in execute_model\n",
      "    output = self.model_runner.execute_model(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 592, in execute_model\n",
      "    outputs = self._final_process_outputs(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 435, in _final_process_outputs\n",
      "    output.pythonize(model_input, self._copy_stream,\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 99, in pythonize\n",
      "    self._pythonize_sampler_output(input_metadata, copy_stream,\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 129, in _pythonize_sampler_output\n",
      "    _pythonize_sampler_output(input_metadata, self.sampler_output,\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/worker/multi_step_model_runner.py\", line 831, in _pythonize_sampler_output\n",
      "    assert len(seq_group.sampling_params.logits_processors) == 0, (\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: Logits Processors are not supported in multi-step decoding\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gcpuser/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/gcpuser/sky_workdir/.venv/lib/python3.12/site-packages/vllm/engine/async_llm_engine.py\", line 70, in _log_task_completion\n",
      "    raise AsyncEngineDeadError(\n",
      "vllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caught exception generating chat completion\n",
      "Logits Processors are not supported in multi-step decoding\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "Logits Processors are not supported in multi-step decoding",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 268\u001b[39m\n\u001b[32m    266\u001b[39m openai_client = \u001b[38;5;28;01mawait\u001b[39;00m model.openai_client()\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;01mawait\u001b[39;00m model.get_step(), \u001b[32m1000\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     train_groups = \u001b[38;5;28;01mawait\u001b[39;00m art.gather_trajectory_groups(\n\u001b[32m    269\u001b[39m         (\n\u001b[32m    270\u001b[39m             art.TrajectoryGroup(\n\u001b[32m    271\u001b[39m                 rollout(openai_client, i, is_validation=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m48\u001b[39m)\n\u001b[32m    272\u001b[39m             )\n\u001b[32m    273\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m)\n\u001b[32m    274\u001b[39m         ),\n\u001b[32m    275\u001b[39m         pbar_desc=\u001b[33m\"\u001b[39m\u001b[33mgather\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m     )\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m model.train(\n\u001b[32m    278\u001b[39m         train_groups,\n\u001b[32m    279\u001b[39m         config=art.TrainConfig(learning_rate=\u001b[32m1e-4\u001b[39m, beta=\u001b[32m0.04\u001b[39m),\n\u001b[32m    280\u001b[39m     )\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m model.delete_checkpoints()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather.py:30\u001b[39m, in \u001b[36mgather_trajectory_groups\u001b[39m\u001b[34m(groups, pbar_desc, pbar_total_completion_tokens, max_exceptions)\u001b[39m\n\u001b[32m     28\u001b[39m     total = \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(g, \u001b[33m\"\u001b[39m\u001b[33m_num_trajectories\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups)\n\u001b[32m     29\u001b[39m     context.pbar = tqdm.tqdm(desc=pbar_desc, total=total)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     result_groups = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     32\u001b[39m     context.pbar.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/gather.py:79\u001b[39m, in \u001b[36mwrap_group_awaitable\u001b[39m\u001b[34m(awaitable)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_group_awaitable\u001b[39m(\n\u001b[32m     76\u001b[39m     awaitable: Awaitable[TrajectoryGroup],\n\u001b[32m     77\u001b[39m ) -> TrajectoryGroup | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(awaitable, \u001b[33m\"\u001b[39m\u001b[33m_num_trajectories\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m awaitable\n\u001b[32m     80\u001b[39m     context = get_gather_context()\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/trajectories.py:103\u001b[39m, in \u001b[36mTrajectoryGroup.__new__.<locals>._\u001b[39m\u001b[34m(exceptions)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m asyncio.as_completed(\n\u001b[32m    100\u001b[39m     cast(\u001b[38;5;28mlist\u001b[39m[Awaitable[Trajectory]], ts)\n\u001b[32m    101\u001b[39m ):\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         trajectory = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    104\u001b[39m         trajectories.append(trajectory)\n\u001b[32m    105\u001b[39m         record_metrics(context, trajectory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:631\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/utils/retry.py:81\u001b[39m, in \u001b[36mretry.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, max_attempts + \u001b[32m1\u001b[39m):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         last_exception = e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 199\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(client, iteration, is_validation)\u001b[39m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m failing_trajectory\n\u001b[32m    198\u001b[39m     failing_trajectory = trajectory\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    202\u001b[39m     op_client.report(\n\u001b[32m    203\u001b[39m         requested_at=requested_at,\n\u001b[32m    204\u001b[39m         received_at=\u001b[38;5;28mint\u001b[39m(time.time() * \u001b[32m1000\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    216\u001b[39m         status_code=\u001b[32m200\u001b[39m,\n\u001b[32m    217\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 190\u001b[39m, in \u001b[36mrollout\u001b[39m\u001b[34m(client, iteration, is_validation)\u001b[39m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    183\u001b[39m         max_completion_tokens=\u001b[32m2048\u001b[39m,\n\u001b[32m    184\u001b[39m         messages=messages,\n\u001b[32m    185\u001b[39m         model=model.name,\n\u001b[32m    186\u001b[39m         extra_body={\u001b[33m\"\u001b[39m\u001b[33mguided_json\u001b[39m\u001b[33m\"\u001b[39m: AgentMove.model_json_schema()},\n\u001b[32m    187\u001b[39m     )\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m get_completion()\n\u001b[32m    191\u001b[39m     last_completion = chat_completion\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.LengthFinishReasonError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mrollout.<locals>.get_completion\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_completion\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    183\u001b[39m         max_completion_tokens=\u001b[32m2048\u001b[39m,\n\u001b[32m    184\u001b[39m         messages=messages,\n\u001b[32m    185\u001b[39m         model=model.name,\n\u001b[32m    186\u001b[39m         extra_body={\u001b[33m\"\u001b[39m\u001b[33mguided_json\u001b[39m\u001b[33m\"\u001b[39m: AgentMove.model_json_schema()},\n\u001b[32m    187\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:55\u001b[39m, in \u001b[36mpatch_openai.<locals>.create_patched\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         context.metric_sums[\u001b[33m\"\u001b[39m\u001b[33mtotal_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[38;5;28msum\u001b[39m(\n\u001b[32m     48\u001b[39m             \u001b[38;5;28mlen\u001b[39m(choice.logprobs.content \u001b[38;5;129;01mor\u001b[39;00m choice.logprobs.refusal \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m     49\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m choice \u001b[38;5;129;01min\u001b[39;00m chunk.choices\n\u001b[32m     50\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m choice.logprobs\n\u001b[32m     51\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m (choice.logprobs.content \u001b[38;5;129;01mor\u001b[39;00m choice.logprobs.refusal)\n\u001b[32m     52\u001b[39m         )\n\u001b[32m     53\u001b[39m         context.update_pbar(n=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m chat_completion = \u001b[38;5;28;01mawait\u001b[39;00m consume_chat_completion_stream(return_value, on_chunk)\n\u001b[32m     56\u001b[39m report_usage(chat_completion)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/src/art/openai.py:85\u001b[39m, in \u001b[36mconsume_chat_completion_stream\u001b[39m\u001b[34m(stream, on_chunk)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Consume a chat completion stream and build a complete ChatCompletion object.\u001b[39;00m\n\u001b[32m     68\u001b[39m \n\u001b[32m     69\u001b[39m \u001b[33;03mThis function processes a stream of ChatCompletionChunks, constructing a complete\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m    AssertionError: If no chat completion object could be created.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m chat_completion: ChatCompletion | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chat_completion \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     87\u001b[39m         chat_completion = ChatCompletion(\n\u001b[32m     88\u001b[39m             \u001b[38;5;28mid\u001b[39m=chunk.id,\n\u001b[32m     89\u001b[39m             choices=[\n\u001b[32m   (...)\u001b[39m\u001b[32m    100\u001b[39m             \u001b[38;5;28mobject\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mchat.completion\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    101\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:147\u001b[39m, in \u001b[36mAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncIterator[_T]:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator:\n\u001b[32m    148\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sky_workdir/.venv/lib/python3.12/site-packages/openai/_streaming.py:174\u001b[39m, in \u001b[36mAsyncStream.__stream__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    171\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    172\u001b[39m             message = \u001b[33m\"\u001b[39m\u001b[33mAn error occurred during streaming\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[32m    175\u001b[39m             message=message,\n\u001b[32m    176\u001b[39m             request=\u001b[38;5;28mself\u001b[39m.response.request,\n\u001b[32m    177\u001b[39m             body=data[\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    178\u001b[39m         )\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m process_data(data=data, cast_to=cast_to, response=response)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAPIError\u001b[39m: Logits Processors are not supported in multi-step decoding"
     ]
    }
   ],
   "source": [
    "import art\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "from typing import TypedDict\n",
    "from openpipe.client import OpenPipe\n",
    "import time\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class TicTacToeGame(TypedDict):\n",
    "    board: list[list[str]]\n",
    "    agent_symbol: Literal[\"x\", \"o\"]\n",
    "    opponent_symbol: Literal[\"x\", \"o\"]\n",
    "\n",
    "\n",
    "def generate_game(board_length: int = 3) -> TicTacToeGame:\n",
    "    board = [[\"_\" for _ in range(board_length)] for _ in range(board_length)]\n",
    "    agent_symbol = random.choice([\"x\", \"o\"])\n",
    "    opponent_symbol = \"x\" if agent_symbol == \"o\" else \"o\"\n",
    "    return {\n",
    "        \"board\": board,\n",
    "        \"agent_symbol\": agent_symbol,\n",
    "        \"opponent_symbol\": opponent_symbol,\n",
    "    }\n",
    "\n",
    "\n",
    "def render_board(game: TicTacToeGame) -> str:\n",
    "    board = game[\"board\"]\n",
    "    board_length = len(board)\n",
    "    # print something like this:\n",
    "    #    1   2   3\n",
    "    # A  _ | x | x\n",
    "    # B  o | _ | _\n",
    "    # C  _ | o | _\n",
    "    # where _ is an empty cell\n",
    "\n",
    "    board_str = \"   \" + \"   \".join([str(i + 1) for i in range(board_length)]) + \"\\n\"\n",
    "    for i in range(board_length):\n",
    "        board_str += f\"{chr(65 + i)}  {board[i][0]} | {board[i][1]} | {board[i][2]}\\n\"\n",
    "    return board_str\n",
    "\n",
    "\n",
    "def get_opponent_move(game: TicTacToeGame) -> tuple[int, int]:\n",
    "    # get a random empty cell\n",
    "    empty_cells = [\n",
    "        (i, j) for i in range(3) for j in range(3) if game[\"board\"][i][j] == \"_\"\n",
    "    ]\n",
    "    return random.choice(empty_cells)\n",
    "\n",
    "\n",
    "class AgentMove(BaseModel):\n",
    "    reason: str\n",
    "    square: str\n",
    "\n",
    "\n",
    "def apply_agent_move(game: TicTacToeGame, move: str) -> None:\n",
    "    board_length = len(game[\"board\"])\n",
    "    json_move = json.loads(move)\n",
    "    square = json_move[\"square\"]\n",
    "\n",
    "    try:\n",
    "        row_index = ord(square[0]) - 65\n",
    "        col_index = int(square[1]) - 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise ValueError(\"Unable to parse square\")\n",
    "\n",
    "    if (\n",
    "        row_index < 0\n",
    "        or row_index >= board_length\n",
    "        or col_index < 0\n",
    "        or col_index >= board_length\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Invalid move, row or column out of bounds: {row_index}, {col_index}\"\n",
    "        )\n",
    "\n",
    "    # check if the move is valid\n",
    "    if game[\"board\"][row_index][col_index] != \"_\":\n",
    "        raise ValueError(\"Square already occupied\")\n",
    "\n",
    "    game[\"board\"][row_index][col_index] = game[\"agent_symbol\"]\n",
    "\n",
    "\n",
    "def check_winner(board: list[list[str]]) -> Literal[\"x\", \"o\", \"draw\", None]:\n",
    "    board_length = len(board)\n",
    "    # check rows\n",
    "    for row in board:\n",
    "        if row.count(row[0]) == board_length and row[0] != \"_\":\n",
    "            return row[0]\n",
    "    # check columns\n",
    "    for col in range(board_length):\n",
    "        if [board[row][col] for row in range(board_length)].count(\n",
    "            board[0][col]\n",
    "        ) == board_length and board[0][col] != \"_\":\n",
    "            return board[0][col]\n",
    "\n",
    "    # top right to bottom left\n",
    "    upward_diagonal = [board[i][board_length - i - 1] for i in range(board_length)]\n",
    "    if (\n",
    "        upward_diagonal.count(upward_diagonal[0]) == board_length\n",
    "        and upward_diagonal[0] != \"_\"\n",
    "    ):\n",
    "        return upward_diagonal[0]\n",
    "\n",
    "    # top left to bottom right\n",
    "    downward_diagonal = [board[i][i] for i in range(board_length)]\n",
    "    if (\n",
    "        downward_diagonal.count(downward_diagonal[0]) == board_length\n",
    "        and downward_diagonal[0] != \"_\"\n",
    "    ):\n",
    "        return downward_diagonal[0]\n",
    "\n",
    "    # check for draw\n",
    "    if all(cell != \"_\" for row in board for cell in row):\n",
    "        return \"draw\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_trajectory_messages(trajectory: art.Trajectory) -> art.Messages:\n",
    "    messages: art.Messages = []\n",
    "    for item in trajectory.messages_and_choices:\n",
    "\n",
    "        # if item is not a dict, convert it to a dict\n",
    "        if not isinstance(item, dict):\n",
    "            item = item.to_dict()\n",
    "\n",
    "        # check if item is a choice\n",
    "        if \"message\" in item:\n",
    "            messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": item[\"message\"][\"content\"]}\n",
    "            )\n",
    "        else:\n",
    "            # otherwise it's a message\n",
    "            messages.append(item)\n",
    "    return messages\n",
    "\n",
    "\n",
    "failing_trajectory = None\n",
    "\n",
    "\n",
    "@art.retry(exceptions=(openai.LengthFinishReasonError,))\n",
    "async def rollout(\n",
    "    client: openai.AsyncOpenAI, iteration: int, is_validation: bool\n",
    ") -> art.Trajectory:\n",
    "\n",
    "    game = generate_game()\n",
    "\n",
    "    trajectory = art.Trajectory(\n",
    "        messages_and_choices=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return the move in the format 'A1', 'B2', 'C3', etc. You are the {game['agent_symbol']} symbol.\",\n",
    "            }\n",
    "        ],\n",
    "        reward=0,\n",
    "        metrics={\"test\": 5},\n",
    "    )\n",
    "\n",
    "    if game[\"agent_symbol\"] == \"o\":\n",
    "        starting_opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
    "            \"opponent_symbol\"\n",
    "        ]\n",
    "\n",
    "    while check_winner(game[\"board\"]) is None:\n",
    "\n",
    "        trajectory.messages_and_choices.append(\n",
    "            {\"role\": \"user\", \"content\": render_board(game)}\n",
    "        )\n",
    "\n",
    "        requested_at = int(time.time() * 1000)\n",
    "        messages = get_trajectory_messages(trajectory)\n",
    "\n",
    "        async def get_completion():\n",
    "            return await client.chat.completions.create(\n",
    "                max_completion_tokens=2048,\n",
    "                messages=messages,\n",
    "                model=model.name,\n",
    "                extra_body={\"guided_json\": AgentMove.model_json_schema()},\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            chat_completion = await get_completion()\n",
    "            last_completion = chat_completion\n",
    "        except openai.LengthFinishReasonError as e:\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(\"caught exception generating chat completion\")\n",
    "            print(e)\n",
    "            global failing_trajectory\n",
    "            failing_trajectory = trajectory\n",
    "            raise e\n",
    "\n",
    "        try:\n",
    "            op_client.report(\n",
    "                requested_at=requested_at,\n",
    "                received_at=int(time.time() * 1000),\n",
    "                req_payload={\n",
    "                    \"model\": model.name,\n",
    "                    \"messages\": messages,\n",
    "                    \"metadata\": {\n",
    "                        \"notebook-id\": \"tic-tac-toe\",\n",
    "                        \"iteration\": str(iteration),\n",
    "                        \"validation\": str(is_validation),\n",
    "                        \"move_number\": str(len(trajectory.messages_and_choices) - 1),\n",
    "                    },\n",
    "                },\n",
    "                resp_payload=chat_completion,\n",
    "                status_code=200,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reporting to OpenPipe: {e}\")\n",
    "\n",
    "        choice = chat_completion.choices[0]\n",
    "        content = choice.message.content\n",
    "        assert isinstance(content, str)\n",
    "        trajectory.messages_and_choices.append(choice)\n",
    "\n",
    "        try:\n",
    "            apply_agent_move(game, content)\n",
    "        except ValueError as e:\n",
    "            trajectory.reward = -1\n",
    "            break\n",
    "\n",
    "        if check_winner(game[\"board\"]) is not None:\n",
    "            break\n",
    "\n",
    "        opponent_move = get_opponent_move(game)\n",
    "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
    "\n",
    "    winner = check_winner(game[\"board\"])\n",
    "\n",
    "    if winner == game[\"agent_symbol\"]:\n",
    "        trajectory.reward = 1\n",
    "    elif winner == game[\"opponent_symbol\"]:\n",
    "        trajectory.reward = 0\n",
    "    elif winner == \"draw\":\n",
    "        trajectory.reward = 0.5\n",
    "\n",
    "    try:\n",
    "        op_client.update_log_metadata(\n",
    "            filters=[\n",
    "                {\n",
    "                    \"field\": \"completionId\",\n",
    "                    \"equals\": last_completion.id,\n",
    "                }\n",
    "            ],\n",
    "            metadata={\n",
    "                \"reward\": str(trajectory.reward),\n",
    "                \"reward_assigned\": \"true\",\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating log metadata: {e}\")\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "openai_client = await model.openai_client()\n",
    "for i in range(await model.get_step(), 1000):\n",
    "    train_groups = await art.gather_trajectory_groups(\n",
    "        (\n",
    "            art.TrajectoryGroup(\n",
    "                rollout(openai_client, i, is_validation=False) for _ in range(48)\n",
    "            )\n",
    "            for _ in range(1)\n",
    "        ),\n",
    "        pbar_desc=\"gather\",\n",
    "    )\n",
    "    await model.train(\n",
    "        train_groups,\n",
    "        config=art.TrainConfig(learning_rate=1e-4, beta=0.04),\n",
    "    )\n",
    "    await model.delete_checkpoints()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
