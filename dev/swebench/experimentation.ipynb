{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-9', 'model-service'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.run([\"pkill\", \"-9\", \"model-service\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "os.environ[\"SWE_AGENT_CONFIG_DIR\"] = \".\"\n",
    "os.environ[\"SWE_AGENT_TOOLS_DIR\"] = \"tools\"\n",
    "os.environ[\"SWE_AGENT_TRAJECTORY_DIR\"] = \"trajectories\"\n",
    "\n",
    "os.makedirs(\"replays\", exist_ok=True)\n",
    "os.makedirs(\"trajectories\", exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instance_id': 'Mimino666__langdetect.a1598f1a.func_pm_class_rm_funcs__842dr37d',\n",
       " 'repo': 'swesmith/Mimino666__langdetect.a1598f1a',\n",
       " 'patch': \"diff --git a/langdetect/detector_factory.py b/langdetect/detector_factory.py\\nindex e026161..b1ab5f4 100644\\n--- a/langdetect/detector_factory.py\\n+++ b/langdetect/detector_factory.py\\n@@ -95,13 +95,6 @@ class DetectorFactory(object):\\n         self.langlist = []\\n         self.word_lang_prob_map = {}\\n \\n-    def create(self, alpha=None):\\n-        '''Construct Detector instance with smoothing parameter.'''\\n-        detector = self._create_detector()\\n-        if alpha is not None:\\n-            detector.set_alpha(alpha)\\n-        return detector\\n-\\n     def _create_detector(self):\\n         if not self.langlist:\\n             raise LangDetectException(ErrorCode.NeedLoadProfileError, 'Need to load profiles.')\\n@@ -113,7 +106,6 @@ class DetectorFactory(object):\\n     def get_lang_list(self):\\n         return list(self.langlist)\\n \\n-\\n PROFILES_DIRECTORY = path.join(path.dirname(__file__), 'profiles')\\n _factory = None\\n \\n\",\n",
       " 'FAIL_TO_PASS': ['langdetect/tests/test_detector.py::DetectorTest::test_detector1',\n",
       "  'langdetect/tests/test_detector.py::DetectorTest::test_detector2',\n",
       "  'langdetect/tests/test_detector.py::DetectorTest::test_detector3',\n",
       "  'langdetect/tests/test_detector.py::DetectorTest::test_detector4'],\n",
       " 'PASS_TO_PASS': ['langdetect/tests/test_detector.py::DetectorTest::test_factory_from_json_string',\n",
       "  'langdetect/tests/test_detector.py::DetectorTest::test_lang_list',\n",
       "  'langdetect/tests/test_language.py::LanguageTest::test_cmp',\n",
       "  'langdetect/tests/test_language.py::LanguageTest::test_language',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_add',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_add_illegally1',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_add_illegally2',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_lang_profile',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_lang_profile_string_int',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_omit_less_freq',\n",
       "  'langdetect/tests/utils/test_lang_profile.py::LangProfileText::test_omit_less_freq_illegally',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_constants',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_ngram',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_ngram3',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_normalize_for_romanian',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_normalize_vietnamese',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_normalize_with_cjk_kanji',\n",
       "  'langdetect/tests/utils/test_ngram.py::NGramTest::test_normalize_with_latin',\n",
       "  'langdetect/tests/utils/test_unicode_block.py::UnicodeBlockTest::test_unicode_block'],\n",
       " 'created_at': datetime.datetime(2025, 3, 15, 5, 22, 47, 264582),\n",
       " 'image_name': 'jyangballin/swesmith.x86_64.mimino666_1776_langdetect.a1598f1a',\n",
       " 'base_commit': 'Mimino666__langdetect.a1598f1a.func_pm_class_rm_funcs__842dr37d',\n",
       " 'problem_statement': \"# Missing `create` method in DetectorFactory\\n\\n## Description\\n\\nI've encountered an issue with the `DetectorFactory` class in langdetect. When trying to create a detector instance, I get an AttributeError.\\n\\n```python\\nfrom langdetect import DetectorFactory\\n\\nfactory = DetectorFactory()\\nfactory.load_profile('path/to/profiles')\\ndetector = factory.create()  # This fails with AttributeError\\n```\\n\\nThe error occurs because the `create` method seems to be missing from the `DetectorFactory` class. This method is essential for creating detector instances with optional smoothing parameters.\\n\\n## Expected behavior\\n\\nThe `create` method should be available in the `DetectorFactory` class to allow creating detector instances with an optional alpha parameter for smoothing.\\n\\n```python\\n# This should work\\nfactory = DetectorFactory()\\nfactory.load_profile('path/to/profiles')\\ndetector = factory.create()  # With default alpha\\ndetector2 = factory.create(alpha=0.5)  # With custom alpha\\n```\\n\\n## Actual behavior\\n\\n```\\nAttributeError: 'DetectorFactory' object has no attribute 'create'\\n```\\n\\nThe `_create_detector` private method exists, but the public `create` method that should call it with the alpha parameter is missing.\",\n",
       " 'use_swebench_modal_harness': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from instances import (\n",
    "    as_instances_iter,\n",
    "    get_filtered_swe_smith_instances_df,\n",
    "    get_swe_bench_verified_instances_df,\n",
    ")\n",
    "\n",
    "# instances_df = get_swe_bench_verified_instances_df()\n",
    "# instances_iter = as_instances_iter(instances_df)\n",
    "# for instance in instances_iter:\n",
    "#     # break\n",
    "#     if (\n",
    "#         # instance[\"difficulty\"] == \"<15 min fix\"\n",
    "#         instance[\"repo\"] not in (\"astropy/astropy\", \"django/django\", \"matplotlib/matplotlib\")\n",
    "#         # and instance[\"instance_id\"] != \"django__django-10097\"\n",
    "#     ):\n",
    "#         break\n",
    "instance = next(get_filtered_swe_smith_instances_df().sample(fraction=1.0, shuffle=True, seed=42).head(1).pipe(as_instances_iter))\n",
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-30 21:04:42 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-30 21:04:42 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/sky_workdir/src/art/__init__.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-30 21:04:49 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-30 21:04:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.1: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.719 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-32b-bnb-4bit with actual GPU utilization = 78.66%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 139.72 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 400.\n",
      "Unsloth: vLLM's KV Cache can use up to 90.04 GB. Also swap space = 6 GB.\n",
      "INFO 05-30 21:04:58 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 05-30 21:04:59 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-32b-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-32b-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-32b-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":400}, use_cached_outputs=False, \n",
      "INFO 05-30 21:04:59 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 05-30 21:04:59 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-30 21:04:59 [model_runner.py:1108] Starting to load model unsloth/qwen3-32b-bnb-4bit...\n",
      "INFO 05-30 21:04:59 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 05-30 21:05:00 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.42it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.30it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.54it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.37it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-30 21:05:06 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 05-30 21:05:06 [model_runner.py:1140] Model loading took 18.1555 GiB and 6.599875 seconds\n",
      "INFO 05-30 21:05:12 [worker.py:287] Memory profiling takes 5.72 seconds\n",
      "INFO 05-30 21:05:12 [worker.py:287] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.79) = 109.90GiB\n",
      "INFO 05-30 21:05:12 [worker.py:287] model weights take 18.16GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 6.53GiB; the rest of the memory reserved for KV Cache is 85.07GiB.\n",
      "INFO 05-30 21:05:12 [executor_base.py:112] # cuda blocks: 21777, # CPU blocks: 1536\n",
      "INFO 05-30 21:05:12 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 10.63x\n",
      "INFO 05-30 21:05:15 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:51<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-30 21:06:06 [model_runner.py:1592] Graph capturing finished in 51 secs, took 3.52 GiB\n",
      "INFO 05-30 21:06:06 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 60.09 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.1 patched 64 layers with 64 QKV layers, 64 O layers and 64 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local import LocalBackend\n",
    "from rollout import ModelConfig, rollout\n",
    "\n",
    "backend = LocalBackend()\n",
    "model = art.TrainableModel(\n",
    "    # name=\"openrouter/google/gemini-2.5-flash-preview-05-20\",\n",
    "    # name=\"openrouter/qwen/qwen3-32b\",\n",
    "    # name=\"Qwen/Qwen3-32B\",\n",
    "    name=\"001\",\n",
    "    project=\"sweagent\",\n",
    "    config=ModelConfig(\n",
    "        max_input_tokens=32_768,\n",
    "        system_prompt_suffix=\"\\n/no_think\",\n",
    "        xml_function_calling=True,\n",
    "    ),\n",
    "    base_model=\"Qwen/Qwen3-32B\",\n",
    ")\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory, run_single = await rollout(model, instance, return_run_single=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.messages_and_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cac4c651cd433aa42b4dd69be505cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gather:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trajectories = await art.gather_trajectories(\n",
    "    (rollout(model, instance) for _ in range(16)),\n",
    "    max_exceptions=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory, run_single = await rollout(\n",
    "    model,\n",
    "    instance,\n",
    "    return_run_single=True,\n",
    "    run_in_thread=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_single.logger.propagate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import Handler, LogRecord\n",
    "\n",
    "class MyHandler(Handler):\n",
    "    def emit(self, record: LogRecord) -> None:\n",
    "        ...\n",
    "\n",
    "run_single.logger.addHandler(MyHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_single.logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_single.logger.warning(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory.messages_and_choices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
