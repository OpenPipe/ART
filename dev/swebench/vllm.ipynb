{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sky/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455\n",
      "INFO 06-12 16:57:11 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 06-12 16:57:13 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-0.6B', speculative_config=None, tokenizer='Qwen/Qwen3-0.6B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-0.6B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "INFO 06-12 16:57:13 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "WARNING 06-12 16:57:13 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x70521cfec7c0>\n",
      "INFO 06-12 16:57:14 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-12 16:57:14 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 06-12 16:57:14 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-12 16:57:14 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-0.6B...\n",
      "INFO 06-12 16:57:14 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 06-12 16:57:14 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n",
      "INFO 06-12 16:57:14 [loader.py:458] Loading weights took 0.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.95it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.94it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-12 16:57:14 [gpu_model_runner.py:1347] Model loading took 1.1201 GiB and 0.377751 seconds\n",
      "INFO 06-12 16:57:15 [kv_cache_utils.py:634] GPU KV cache size: 1,107,856 tokens\n",
      "INFO 06-12 16:57:15 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 27.05x\n",
      "INFO 06-12 16:57:15 [core.py:159] init engine (profile, create kv cache, warmup model) took 0.53 seconds\n"
     ]
    }
   ],
   "source": [
    "from art.multidevice.vllm import get_llm\n",
    "import vllm\n",
    "\n",
    "llm = await get_llm(\n",
    "    vllm.AsyncEngineArgs(\n",
    "        model=\"Qwen/Qwen3-0.6B\",\n",
    "        # pipeline_parallel_size=1,\n",
    "        # tensor_parallel_size=2,\n",
    "        # data_parallel_size=2,\n",
    "        enforce_eager=True,\n",
    "        generation_config=\"vllm\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-12 16:57:20] INFO _client.py:1740: HTTP Request: GET http://0.0.0.0:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local.vllm import openai_server_task\n",
    "\n",
    "task = await openai_server_task(\n",
    "    engine=llm,\n",
    "    config=art.dev.OpenAIServerConfig(\n",
    "        log_file=\"./vllm.log\",\n",
    "        server_args=art.dev.ServerArgs(\n",
    "            api_key=\"default\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            enable_auto_tool_choice=True,\n",
    "            tool_call_parser=\"hermes\",\n",
    "        ),\n",
    "        engine_args=art.dev.EngineArgs(\n",
    "            model=\"Qwen/Qwen3-0.6B\",\n",
    "            served_model_name=\"Qwen/Qwen3-0.6B\",\n",
    "            disable_log_requests=True,\n",
    "            generation_config=\"vllm\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W612 16:58:59.025217257 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "llm.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
