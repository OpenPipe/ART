{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-13 03:50:39 [__init__.py:239] Automatically detected platform cuda.\n",
      "/home/sky/.cache/huggingface/hub/models--Qwen--Qwen3-32B/snapshots/d47b0d4ae4b48fde975756bf360a63a9cca8d470\n",
      "INFO 06-13 03:50:47 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 06-13 03:50:47 [config.py:1770] Defaulting to use mp for distributed inference\n",
      "INFO 06-13 03:50:47 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 06-13 03:50:47 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 06-13 03:50:48 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
      "WARNING 06-13 03:50:48 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 10485760, 10, 'psm_82748d0e'), local_subscribe_addr='ipc:///tmp/5601b6c7-ffb1-4523-b129-5fcabdb6627b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3b557a90>\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f023a56e'), local_subscribe_addr='ipc:///tmp/2b23643f-3ab8-40eb-954d-30112eddc41b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a359b10>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4e7fb02e'), local_subscribe_addr='ipc:///tmp/6bad4a45-8177-4759-bba3-759997defd89', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a358e80>\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_595bf248'), local_subscribe_addr='ipc:///tmp/9630ae54-dc32-4263-9fae-26c9344e674f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a358b80>\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fd75af81'), local_subscribe_addr='ipc:///tmp/6deebd18-325d-43fc-9ad6-f7438b1866cf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a3581c0>\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fc1eaef2'), local_subscribe_addr='ipc:///tmp/079dd2d9-f639-47ac-9190-ef54c43831e8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-13 03:50:48 [worker_base.py:589] Injected <class 'art.multidevice.vllm.WorkerExtension'> into <class 'vllm.v1.worker.gpu_worker.Worker'> for extended collective_rpc calls ['run']\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a358430>\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_87b42572'), local_subscribe_addr='ipc:///tmp/156c0a6c-4a55-4987-9989-60ab4b0fa2f6', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a358370>\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0cda4486'), local_subscribe_addr='ipc:///tmp/8ac1f488-41ea-4ea5-b679-5e8ed66a9254', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-13 03:50:48 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x700d3a359390>\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m INFO 06-13 03:50:48 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a70a2004'), local_subscribe_addr='ipc:///tmp/3f9029bd-1228-40d2-93b2-95dc19d2252d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W613 03:50:55.902934869 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:55.903389659 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:57.314434967 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:57.314805519 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:57.857940216 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:57.858379903 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.087940145 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.088123079 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.088323159 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.088536078 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.093267191 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.093463417 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.277155476 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.277464020 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:50:58 [utils.py:1055] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:50:58 [pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W613 03:50:58.306449802 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W613 03:50:58.306666851 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:51:06 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/sky/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3,4,5,6,7.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:51:06 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_0fb223c5'), local_subscribe_addr='ipc:///tmp/4d2464d8-bfae-4cc6-a975-656c073f5e44', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:51:06 [parallel_state.py:1004] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4\n",
      "INFO 06-13 03:51:06 [parallel_state.py:1004] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m \u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-13 03:51:06 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-13 03:51:06 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m \u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m \u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m \u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:51:06 [gpu_model_runner.py:1329] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:51:06 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62caa8beb7aa40efa3e778d3d2b8ad6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.63 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.65 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.62 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:51:09 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 2.904101 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.58 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.74 seconds\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m INFO 06-13 03:51:09 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 3.101532 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:51:09 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 3.111612 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m INFO 06-13 03:51:09 [loader.py:458] Loading weights took 2.66 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:51:10 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 3.150724 seconds\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:51:10 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 3.236053 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:51:10 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 3.274857 seconds\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m INFO 06-13 03:51:10 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 3.369699 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:51:16 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:51:19 [loader.py:458] Loading weights took 2.56 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:51:19 [gpu_model_runner.py:1347] Model loading took 7.6871 GiB and 12.852402 seconds\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,501,056 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.47x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,497,984 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.40x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,497,984 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.40x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,497,984 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.40x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,497,984 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.40x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,497,984 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.40x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,497,984 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.40x\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:634] GPU KV cache size: 3,513,344 tokens\n",
      "INFO 06-13 03:51:26 [kv_cache_utils.py:637] Maximum concurrency for 40,960 tokens per request: 85.78x\n",
      "INFO 06-13 03:51:26 [core.py:159] init engine (profile, create kv cache, warmup model) took 7.22 seconds\n",
      "INFO 06-13 03:51:27 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.50 GiB memory, 19.64 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.62 GiB memory, 19.55 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.50 GiB memory, 19.64 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.50 GiB memory, 19.64 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.50 GiB memory, 19.64 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.50 GiB memory, 19.64 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 107.00 GiB memory, 19.17 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m INFO 06-13 03:52:38 [gpu_worker.py:95] Sleep mode freed 106.50 GiB memory, 19.64 GiB memory is still in use.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470]     raise\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=72074)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]     raise\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=2 pid=72061)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470]     raise\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=72058)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470]     raise\n",
      "ERROR 06-13 03:53:12 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=72055)\u001b[0;0m ERROR 06-13 03:53:12 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "ERROR 06-13 03:53:12 [core.py:459] Invocation of sleep method failed\n",
      "ERROR 06-13 03:53:12 [core.py:459] Traceback (most recent call last):\n",
      "ERROR 06-13 03:53:12 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 456, in _handle_client_request\n",
      "ERROR 06-13 03:53:12 [core.py:459]     output.result = method(\n",
      "ERROR 06-13 03:53:12 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 268, in sleep\n",
      "ERROR 06-13 03:53:12 [core.py:459]     self.model_executor.sleep(level)\n",
      "ERROR 06-13 03:53:12 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 206, in sleep\n",
      "ERROR 06-13 03:53:12 [core.py:459]     self.collective_rpc(\"sleep\", kwargs=dict(level=level))\n",
      "ERROR 06-13 03:53:12 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 185, in collective_rpc\n",
      "ERROR 06-13 03:53:12 [core.py:459]     raise RuntimeError(\n",
      "ERROR 06-13 03:53:12 [core.py:459] RuntimeError: Worker failed with error 'No active exception to reraise', please check the stack trace above for the root cause\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]     raise\n",
      "\u001b[1;36m(VllmWorker rank=5 pid=72092)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]     raise\n",
      "\u001b[1;36m(VllmWorker rank=4 pid=72080)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "\u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] WorkerProc hit an exception.\n",
      "ERROR 06-13 03:53:13 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] Traceback (most recent call last):\n",
      "ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 465, in worker_busy_loop\n",
      "ERROR 06-13 03:53:13 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]     output = func(*args, **kwargs)\n",
      "ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]   File \"/home/sky/sky_workdir/src/art/multidevice/vllm.py\", line 139, in sleep\n",
      "ERROR 06-13 03:53:13 [multiproc_executor.py:470]     raise\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m \u001b[1;36m(VllmWorker rank=6 pid=72105)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470]     raise\n",
      "\u001b[1;36m(VllmWorker rank=7 pid=72121)\u001b[0;0m ERROR 06-13 03:53:13 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "ERROR 06-13 03:53:13 [multiproc_executor.py:470] RuntimeError: No active exception to reraise\n",
      "ERROR 06-13 03:53:42 [core.py:459] Invocation of collective_rpc method failed\n",
      "ERROR 06-13 03:53:42 [core.py:459] Traceback (most recent call last):\n",
      "ERROR 06-13 03:53:42 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 456, in _handle_client_request\n",
      "ERROR 06-13 03:53:42 [core.py:459]     output.result = method(\n",
      "ERROR 06-13 03:53:42 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 306, in collective_rpc\n",
      "ERROR 06-13 03:53:42 [core.py:459]     return self.model_executor.collective_rpc(method, timeout, args,\n",
      "ERROR 06-13 03:53:42 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 185, in collective_rpc\n",
      "ERROR 06-13 03:53:42 [core.py:459]     raise RuntimeError(\n",
      "ERROR 06-13 03:53:42 [core.py:459] RuntimeError: Worker failed with error 'No active exception to reraise', please check the stack trace above for the root cause\n",
      "ERROR 06-13 03:53:51 [core.py:459] Invocation of collective_rpc method failed\n",
      "ERROR 06-13 03:53:51 [core.py:459] Traceback (most recent call last):\n",
      "ERROR 06-13 03:53:51 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 456, in _handle_client_request\n",
      "ERROR 06-13 03:53:51 [core.py:459]     output.result = method(\n",
      "ERROR 06-13 03:53:51 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 306, in collective_rpc\n",
      "ERROR 06-13 03:53:51 [core.py:459]     return self.model_executor.collective_rpc(method, timeout, args,\n",
      "ERROR 06-13 03:53:51 [core.py:459]   File \"/home/sky/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/executor/multiproc_executor.py\", line 185, in collective_rpc\n",
      "ERROR 06-13 03:53:51 [core.py:459]     raise RuntimeError(\n",
      "ERROR 06-13 03:53:51 [core.py:459] RuntimeError: Worker failed with error 'No active exception to reraise', please check the stack trace above for the root cause\n"
     ]
    }
   ],
   "source": [
    "from art.multidevice.vllm import get_llm\n",
    "import torch\n",
    "import vllm\n",
    "\n",
    "llm = await get_llm(\n",
    "    vllm.AsyncEngineArgs(\n",
    "        model=\"Qwen/Qwen3-32B\",\n",
    "        # pipeline_parallel_size=1,\n",
    "        tensor_parallel_size=torch.cuda.device_count(),\n",
    "        # data_parallel_size=1,\n",
    "        enforce_eager=True,\n",
    "        generation_config=\"vllm\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-13 03:51:30] INFO config.py:54: PyTorch version 2.6.0 available.\n",
      "[2025-06-13 03:51:30] INFO config.py:66: Polars version 1.30.0 available.\n",
      "[2025-06-13 03:51:31] INFO _client.py:1740: HTTP Request: GET http://0.0.0.0:8000/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local.vllm import openai_server_task\n",
    "\n",
    "task = await openai_server_task(\n",
    "    engine=llm,\n",
    "    config=art.dev.OpenAIServerConfig(\n",
    "        log_file=\"./vllm.log\",\n",
    "        server_args=art.dev.ServerArgs(\n",
    "            api_key=\"default\",\n",
    "            return_tokens_as_token_ids=True,\n",
    "            enable_auto_tool_choice=True,\n",
    "            tool_call_parser=\"hermes\",\n",
    "        ),\n",
    "        engine_args=art.dev.EngineArgs(\n",
    "            model=llm.vllm_config.model_config.model,\n",
    "            disable_log_requests=True,\n",
    "            generation_config=\"vllm\",\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Call to collective_rpc method failed: Worker failed with error 'No active exception to reraise', please check the stack trace above for the root cause",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m pids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_on_workers(llm, \u001b[38;5;28;01mlambda\u001b[39;00m: os\u001b[38;5;241m.\u001b[39mgetpid())\n\u001b[1;32m      7\u001b[0m sleep_task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(llm\u001b[38;5;241m.\u001b[39msleep())\n",
      "File \u001b[0;32m~/sky_workdir/src/art/multidevice/vllm.py:151\u001b[0m, in \u001b[0;36mrun_on_workers\u001b[0;34m(llm, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_on_workers\u001b[39m(\n\u001b[1;32m    149\u001b[0m     llm: AsyncLLM, func: Callable[P, R], \u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m    150\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[R]:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mcollective_rpc(\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39m(cloudpickle\u001b[38;5;241m.\u001b[39mdumps(func), \u001b[38;5;241m*\u001b[39margs), kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    153\u001b[0m     )\n",
      "File \u001b[0;32m~/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/async_llm.py:514\u001b[0m, in \u001b[0;36mAsyncLLM.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollective_rpc\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m                          method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    508\u001b[0m                          timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                          args: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m    510\u001b[0m                          kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m    Perform a collective RPC call to the given path.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mcollective_rpc_async(\n\u001b[1;32m    515\u001b[0m         method, timeout, args, kwargs)\n",
      "File \u001b[0;32m~/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:822\u001b[0m, in \u001b[0;36mAsyncMPClient.collective_rpc_async\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollective_rpc_async\u001b[39m(\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    818\u001b[0m         method: Union[\u001b[38;5;28mstr\u001b[39m, Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, _R]],\n\u001b[1;32m    819\u001b[0m         timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    820\u001b[0m         args: \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;241m=\u001b[39m (),\n\u001b[1;32m    821\u001b[0m         kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[_R]:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_utility_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollective_rpc\u001b[39m\u001b[38;5;124m\"\u001b[39m, method, timeout,\n\u001b[1;32m    823\u001b[0m                                          args, kwargs)\n",
      "File \u001b[0;32m~/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:756\u001b[0m, in \u001b[0;36mAsyncMPClient.call_utility_async\u001b[0;34m(self, method, *args)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_utility_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_utility_async(method,\n\u001b[1;32m    757\u001b[0m                                           \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    758\u001b[0m                                           engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_engine)\n",
      "File \u001b[0;32m~/sky_workdir/.venv/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:769\u001b[0m, in \u001b[0;36mAsyncMPClient._call_utility_async\u001b[0;34m(self, method, engine, *args)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_input_message(message, engine, args)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_output_queue_task()\n\u001b[0;32m--> 769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/asyncio/futures.py:285\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/asyncio/tasks.py:304\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mException\u001b[0m: Call to collective_rpc method failed: Worker failed with error 'No active exception to reraise', please check the stack trace above for the root cause"
     ]
    }
   ],
   "source": [
    "from art.multidevice.vllm import run_on_workers\n",
    "import asyncio\n",
    "import os\n",
    "import signal\n",
    "\n",
    "pids = await run_on_workers(llm, lambda: os.getpid())\n",
    "sleep_task = asyncio.create_task(llm.sleep())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await llm.is_sleeping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in pids:\n",
    "    os.kill(pid, signal.SIGCONT)\n",
    "# await sleep_task\n",
    "# await llm.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_task.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid in pids:\n",
    "    os.kill(pid, signal.SIGCONT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.sleep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await llm.wake_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
