{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "os.environ[\"SWE_AGENT_CONFIG_DIR\"] = \".\"\n",
    "os.environ[\"SWE_AGENT_TOOLS_DIR\"] = \"tools\"\n",
    "os.environ[\"SWE_AGENT_TRAJECTORY_DIR\"] = \"trajectories\"\n",
    "\n",
    "os.makedirs(\"replays\", exist_ok=True)\n",
    "os.makedirs(\"trajectories\", exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 17:52:44 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-03 17:52:44 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/sky_workdir/src/art/__init__.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-03 17:52:51 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-03 17:52:51 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.1: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.719 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-32b-bnb-4bit with actual GPU utilization = 78.66%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 139.72 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 400.\n",
      "Unsloth: vLLM's KV Cache can use up to 90.04 GB. Also swap space = 6 GB.\n",
      "INFO 06-03 17:53:01 [config.py:717] This model supports multiple tasks: {'generate', 'classify', 'score', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 06-03 17:53:01 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-32b-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-32b-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-32b-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":400}, use_cached_outputs=False, \n",
      "INFO 06-03 17:53:01 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 06-03 17:53:02 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-03 17:53:02 [model_runner.py:1108] Starting to load model unsloth/qwen3-32b-bnb-4bit...\n",
      "INFO 06-03 17:53:02 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 06-03 17:53:02 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.51it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.43it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.37it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.33it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 17:53:08 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 06-03 17:53:08 [model_runner.py:1140] Model loading took 18.1555 GiB and 6.536463 seconds\n",
      "INFO 06-03 17:53:14 [worker.py:287] Memory profiling takes 5.72 seconds\n",
      "INFO 06-03 17:53:14 [worker.py:287] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.79) = 109.90GiB\n",
      "INFO 06-03 17:53:14 [worker.py:287] model weights take 18.16GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 6.53GiB; the rest of the memory reserved for KV Cache is 85.07GiB.\n",
      "INFO 06-03 17:53:15 [executor_base.py:112] # cuda blocks: 21777, # CPU blocks: 1536\n",
      "INFO 06-03 17:53:15 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 10.63x\n",
      "INFO 06-03 17:53:18 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 17:54:08 [model_runner.py:1592] Graph capturing finished in 50 secs, took 3.52 GiB\n",
      "INFO 06-03 17:54:08 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 59.12 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.1 patched 64 layers with 64 QKV layers, 64 O layers and 64 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local import LocalBackend\n",
    "from rollout import ModelConfig\n",
    "\n",
    "backend = LocalBackend()\n",
    "model = art.TrainableModel(\n",
    "    name=\"001\",\n",
    "    project=\"sweagent\",\n",
    "    config=ModelConfig(\n",
    "        max_input_tokens=32_768,\n",
    "        system_prompt_suffix=\"\\n/no_think\",\n",
    "        xml_function_calling=True,\n",
    "    ),\n",
    "    base_model=\"Qwen/Qwen3-32B\",\n",
    ")\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import modal\n",
    "\n",
    "sandboxes: list[modal.Sandbox] = []\n",
    "async for sandbox in modal.Sandbox.list.aio(\n",
    "    app_id=modal.App.lookup(\"swe-rex\", create_if_missing=True).app_id\n",
    "):\n",
    "    sandboxes.append(sandbox)\n",
    "_ = await asyncio.gather(*[sandbox.terminate.aio() for sandbox in sandboxes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e1c5f84db04991bfd35a0aaa67cd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_result: {'num_failed_f2p': 0, 'num_passed_f2p': 0, 'num_failed_p2p': 0, 'num_passed_p2p': 0}, instance_id: facebookresearch__hydra.0f03eb60.lm_rewrite__djqcu4h3\n",
      "Container process terminated.\n",
      "stdout:\n",
      "INFO:     172.20.0.1:36618 - \"GET /is_alive HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36622 - \"POST /create_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36634 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36646 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36662 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36668 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:40536 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:40538 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:40546 - \"POST /upload HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /execute HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /execute HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /write_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /write_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:36680 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:54336 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:54336 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:54336 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:39780 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:39780 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:39780 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:42538 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:42538 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:42538 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:44902 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:44902 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:44902 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:33122 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:33122 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:33122 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:59522 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:59522 - \"POST /run_in_session HTTP/1.1\" 200 OK\n",
      "INFO:     172.20.0.1:59522 - \"POST /read_file HTTP/1.1\" 200 OK\n",
      "\n",
      "stderr:\n",
      "bash: line 1: swerex-remote: command not found\n",
      "creating virtual environment...\n",
      "creating shared libraries...\n",
      "upgrading shared libraries...\n",
      "installing swe-rex...\n",
      "INFO:     Started server process [2]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8880 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [2]\n",
      "\n",
      "eval_result: {'num_failed_f2p': 0, 'num_passed_f2p': 0, 'num_failed_p2p': 0, 'num_passed_p2p': 0}, instance_id: facebookresearch__hydra.0f03eb60.lm_rewrite__djqcu4h3\n",
      "eval_result: {'num_failed_f2p': 0, 'num_passed_f2p': 0, 'num_failed_p2p': 0, 'num_passed_p2p': 0}, instance_id: facebookresearch__hydra.0f03eb60.lm_rewrite__djqcu4h3\n",
      "eval_result: {'num_failed_f2p': 0, 'num_passed_f2p': 0, 'num_failed_p2p': 0, 'num_passed_p2p': 0}, instance_id: facebookresearch__hydra.0f03eb60.lm_rewrite__djqcu4h3\n",
      "HTTPSConnectionPool(host='85xxso9i8b6cs2.r27.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='lryjf8fof4fn9x.r23.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='nr2hwrd3ab6b0a.r27.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='bkn7794lp0gphv.r27.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='8ojftnm8lrt98q.r26.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='k257u7fibch423.r23.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbradhilton\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sky/sky_workdir/dev/swebench/wandb/run-20250603_183204-001</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/bradhilton/sweagent/runs/001' target=\"_blank\">001</a></strong> to <a href='https://wandb.ai/bradhilton/sweagent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bradhilton/sweagent' target=\"_blank\">https://wandb.ai/bradhilton/sweagent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bradhilton/sweagent/runs/001' target=\"_blank\">https://wandb.ai/bradhilton/sweagent/runs/001</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wandb run initialized! You can view it at https://wandb.ai/bradhilton/sweagent/runs/001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8546fe66c124a6287ed624de35186d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9306613762d54af5a90a25c46cef83ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578d8b9738454b14971fc5b0b1a57635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df030356b47e4fb3930193e8c83df50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no assistant logprobs to train on. Did you forget to include at least one Choice in Trajectory.messages_and_choices?\n",
      "Skipping tuning as there is no suitable data. This can happen when all the trajectories in the same group have the same reward and thus no advantage to train on.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721622dbd2d142f4b3ac671a9d1c9b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='d41ivkzszkf4qg.r38.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='gy06fbrsmu19nx.r37.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='9nupbp45qvfagr.r30.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n",
      "HTTPSConnectionPool(host='1t37ip7a0a4rmo.r33.modal.host', port=443): Max retries exceeded with url: /run_in_session (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\n"
     ]
    }
   ],
   "source": [
    "from instances import as_instances_iter, get_filtered_swe_smith_instances_df\n",
    "from rollout import rollout\n",
    "\n",
    "tasks = (\n",
    "    get_filtered_swe_smith_instances_df()\n",
    "    .sample(fraction=1.0, shuffle=True, seed=42)\n",
    "    .pipe(as_instances_iter)\n",
    ")\n",
    "\n",
    "async for batch in art.trajectory_group_batches(\n",
    "    (art.TrajectoryGroup(rollout(model, task) for _ in range(4)) for task in tasks),\n",
    "    batch_size=4,\n",
    "    # max_batch_exceptions=128,\n",
    "    max_concurrent_batches=4,\n",
    "    skip_batches=await model.get_step(),\n",
    "):\n",
    "    await model.train(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
