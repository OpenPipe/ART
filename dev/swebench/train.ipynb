{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".cell-output-ipywidget-background {\n",
       "    background-color: transparent !important;\n",
       "}\n",
       ":root {\n",
       "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "}  \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import os\n",
    "\n",
    "os.environ[\"SWE_AGENT_CONFIG_DIR\"] = \".\"\n",
    "os.environ[\"SWE_AGENT_TOOLS_DIR\"] = \"tools\"\n",
    "os.environ[\"SWE_AGENT_TRAJECTORY_DIR\"] = \"trajectories\"\n",
    "\n",
    "os.makedirs(\"replays\", exist_ok=True)\n",
    "os.makedirs(\"trajectories\", exist_ok=True)\n",
    "\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-31 16:56:11 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-31 16:56:11 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sky/sky_workdir/src/art/__init__.py:11: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  import unsloth  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-31 16:56:18 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-31 16:56:18 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.1: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA H200. Num GPUs = 1. Max memory: 139.719 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-32b-bnb-4bit with actual GPU utilization = 78.66%\n",
      "Unsloth: Your GPU has CUDA compute capability 9.0 with VRAM = 139.72 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 32768. Num Sequences = 400.\n",
      "Unsloth: vLLM's KV Cache can use up to 90.04 GB. Also swap space = 6 GB.\n",
      "INFO 05-31 16:56:27 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 05-31 16:56:27 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-32b-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-32b-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-32b-bnb-4bit, num_scheduler_steps=16, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":400}, use_cached_outputs=False, \n",
      "INFO 05-31 16:56:27 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 05-31 16:56:28 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-31 16:56:28 [model_runner.py:1108] Starting to load model unsloth/qwen3-32b-bnb-4bit...\n",
      "INFO 05-31 16:56:28 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 05-31 16:56:28 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.32it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.31it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.27it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.29it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-31 16:56:35 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 05-31 16:56:35 [model_runner.py:1140] Model loading took 18.1555 GiB and 6.866419 seconds\n",
      "INFO 05-31 16:56:41 [worker.py:287] Memory profiling takes 5.86 seconds\n",
      "INFO 05-31 16:56:41 [worker.py:287] the current vLLM instance can use total_gpu_memory (139.72GiB) x gpu_memory_utilization (0.79) = 109.90GiB\n",
      "INFO 05-31 16:56:41 [worker.py:287] model weights take 18.16GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 6.53GiB; the rest of the memory reserved for KV Cache is 85.07GiB.\n",
      "INFO 05-31 16:56:41 [executor_base.py:112] # cuda blocks: 21777, # CPU blocks: 1536\n",
      "INFO 05-31 16:56:41 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 10.63x\n",
      "INFO 05-31 16:56:44 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:49<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-31 16:57:34 [model_runner.py:1592] Graph capturing finished in 50 secs, took 3.52 GiB\n",
      "INFO 05-31 16:57:34 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 58.95 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.1 patched 64 layers with 64 QKV layers, 64 O layers and 64 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "import art\n",
    "from art.local import LocalBackend\n",
    "from rollout import ModelConfig\n",
    "\n",
    "backend = LocalBackend()\n",
    "model = art.TrainableModel(\n",
    "    name=\"001\",\n",
    "    project=\"sweagent\",\n",
    "    config=ModelConfig(\n",
    "        max_input_tokens=32_768,\n",
    "        system_prompt_suffix=\"\\n/no_think\",\n",
    "        xml_function_calling=True,\n",
    "    ),\n",
    "    base_model=\"Qwen/Qwen3-32B\",\n",
    ")\n",
    "await model.register(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e02e89c52af4f4181b913ccf48390ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:rex-deploy-asyncio_5:Traceback: \n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/swerex/runtime/local.py\", line 310, in _run_normal\n",
      "    expect_index = self.shell.expect(expect_strings, timeout=action.timeout)  # type: ignore\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/pexpect/spawnbase.py\", line 354, in expect\n",
      "    return self.expect_list(compiled_pattern_list,\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/pexpect/spawnbase.py\", line 383, in expect_list\n",
      "    return exp.expect_loop(timeout)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/pexpect/expect.py\", line 181, in expect_loop\n",
      "    return self.timeout(e)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/pexpect/expect.py\", line 144, in timeout\n",
      "    raise exc\n",
      "pexpect.exceptions.TIMEOUT: Timeout exceeded.\n",
      "<pexpect.pty_spawn.spawn object at 0x2adece5ce290>\n",
      "command: /usr/bin/env\n",
      "args: [b'/usr/bin/env', b'bash']\n",
      "buffer (last 100 chars): 'ction WSGI server instead.\\x1b[0m\\r\\n * Running on http://127.0.0.1:5000\\r\\n\\x1b[33mPress CTRL+C to quit\\x1b[0m\\r\\n'\n",
      "before (last 100 chars): 'ction WSGI server instead.\\x1b[0m\\r\\n * Running on http://127.0.0.1:5000\\r\\n\\x1b[33mPress CTRL+C to quit\\x1b[0m\\r\\n'\n",
      "after: <class 'pexpect.exceptions.TIMEOUT'>\n",
      "match: None\n",
      "match_index: None\n",
      "exitstatus: None\n",
      "flag_eof: False\n",
      "pid: 31\n",
      "child_fd: 10\n",
      "closed: False\n",
      "timeout: 30\n",
      "delimiter: <class 'pexpect.exceptions.EOF'>\n",
      "logfile: None\n",
      "logfile_read: None\n",
      "logfile_send: None\n",
      "maxread: 2000\n",
      "ignorecase: False\n",
      "searchwindowsize: None\n",
      "delaybeforesend: 0.05\n",
      "delayafterclose: 0.1\n",
      "delayafterterminate: 0.1\n",
      "searcher: searcher_re:\n",
      "    0: re.compile('SHELLPS1PREFIX')\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/middleware/base.py\", line 176, in __call__\n",
      "    with recv_stream, send_stream, collapse_excgroups():\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n",
      "    raise exc\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/middleware/base.py\", line 178, in __call__\n",
      "    response = await self.dispatch_func(request, call_next)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/swerex/server.py\", line 48, in authenticate\n",
      "    return await call_next(request)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/middleware/base.py\", line 156, in call_next\n",
      "    raise app_exc\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/middleware/base.py\", line 141, in coro\n",
      "    await self.app(scope, receive_or_disconnect, send_no_error)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/routing.py\", line 714, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/routing.py\", line 734, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/swerex/server.py\", line 85, in run\n",
      "    return serialize_model(await runtime.run_in_session(action))\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/swerex/runtime/local.py\", line 406, in run_in_session\n",
      "    return await self.sessions[action.session].run(action)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/swerex/runtime/local.py\", line 235, in run\n",
      "    r = await self._run_normal(action)\n",
      "  File \"/root/.local/pipx/.cache/eac1061a7f38818/lib/python3.10/site-packages/swerex/runtime/local.py\", line 314, in _run_normal\n",
      "    raise CommandTimeoutError(msg) from e\n",
      "swerex.exceptions.CommandTimeoutError: timeout after 30.0 seconds while running command 'cd /testbed && python -m moto.server --host 127.0.0.1 --port 5000'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swerex.exceptions CommandTimeoutError\n"
     ]
    }
   ],
   "source": [
    "from instances import as_instances_iter, get_filtered_swe_smith_instances_df\n",
    "from rollout import rollout\n",
    "\n",
    "tasks = (\n",
    "    get_filtered_swe_smith_instances_df()\n",
    "    .sample(fraction=1.0, shuffle=True, seed=42)\n",
    "    .pipe(as_instances_iter)\n",
    ")\n",
    "\n",
    "async for batch in art.trajectory_group_batches(\n",
    "    (art.TrajectoryGroup(rollout(model, task) for _ in range(4)) for task in tasks),\n",
    "    batch_size=4,\n",
    "    # max_batch_exceptions=4,\n",
    "    max_concurrent_batches=4,\n",
    "    skip_batches=await model.get_step(),\n",
    "):\n",
    "    await model.train(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
